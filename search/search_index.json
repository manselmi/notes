{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Mike Anselmi's notes","text":""},{"location":"#project-url","title":"Project URL","text":"<ul> <li> <p>Git repository</p> </li> <li> <p>Published site</p> </li> </ul>"},{"location":"#introduction","title":"Introduction","text":"<p>Here are some of my technical notes presented with Material for MkDocs, chosen for its design principles:</p> <ul> <li> <p>It's just Markdown: Focus on the content of your documentation and create a professional   static site in minutes. No need to know HTML, CSS or JavaScript \u2013 let Material for MkDocs do   the heavy lifting for you.</p> </li> <li> <p>Works on all devices: Serve your documentation with confidence \u2013 the underlying layout   automatically adapts to perfectly fit the available screen estate, no matter the type or size of   the viewing device.</p> </li> <li> <p>Made to measure: Change the colors, fonts, language, icons, logo and much more with a few   lines of configuration. Material for MkDocs can be easily extended and provides tons of options   to alter appearance and behavior.</p> </li> <li> <p>Fast and lightweight: Don't let your users wait \u2013 get incredible value with a small   footprint, by using one of the fastest themes around with excellent performance, yielding great   search engine rankings and happy users that return.</p> </li> <li> <p>Accessible: Make accessibility a priority \u2013 users can navigate your documentation with   touch devices, keyboard, and screen readers. Semantic markup ensures that your documentation   works for everyone.</p> </li> <li> <p>Open Source: Trust 45,000+ users \u2013 choose a mature and well-funded solution built with   state-of-the-art Open Source technologies. Keep ownership of your content without fear of vendor   lock-in. Licensed under MIT.</p> </li> </ul> <p>More specifically, I use the sponsorware edition Material for MkDocs Insiders:</p> <p>Material for MkDocs follows the sponsorware release strategy, which means that new features are first exclusively released to sponsors as part of Insiders. Read on to learn what sponsorships achieve, how to become a sponsor to get access to Insiders, and what's in it for you!</p> <p>What is Insiders?</p> <p>Material for MkDocs Insiders is a private fork of Material for MkDocs, hosted as a private GitHub repository. Almost all new features are developed as part of this fork, which means that they are immediately available to all eligible sponsors, as they are made collaborators of this repository.</p> <p>Every feature is tied to a funding goal in monthly subscriptions. When a funding goal is hit, the features that are tied to it are merged back into Material for MkDocs and released for general availability, making them available to all users. Bugfixes are always released in tandem.</p>"},{"location":"aws-vault/","title":"aws-vault","text":"","tags":["aws","security"]},{"location":"aws-vault/#project-url","title":"Project URL","text":"<p>aws-vault</p>","tags":["aws","security"]},{"location":"aws-vault/#use-case","title":"Use case","text":"<p>I serve static content from the domain <code>static.manselmi.com</code>. Objects reside in a bucket in <code>us-east-1</code> named <code>static.manselmi.com</code>, and objects within that bucket are accessible only via a CloudFront distribution.</p> <p>I would like to be able to do the following programatically:</p> <ul> <li> <p>get, put and delete objects within the <code>static.manselmi.com</code> bucket</p> </li> <li> <p>invalidate the CloudFront cache (e.g. I just overwrote an object and want to ensure CloudFront   doesn't serve a previously cached object)</p> </li> <li> <p>rotate long-term credentials</p> </li> </ul>","tags":["aws","security"]},{"location":"aws-vault/#configuration","title":"Configuration","text":"<ol> <li> <p>Create IAM user <code>manselmi-work-sts</code>. The user is so named because it's for my occasional use at    work, and will only be able to do anything useful by assuming a privileged role via STS.</p> </li> <li> <p>Create IAM user group <code>work</code>, and assign the user <code>manselmi-work-sts</code> to the group.</p> </li> <li> <p>Create IAM permission policy <code>static.manselmi.com</code> that allows the required S3 and CloudFront    actions.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ManselmiAllowS3BucketStaticManselmiCom\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketPolicy\",\n                \"s3:GetBucketPolicyStatus\",\n                \"s3:GetBucketPublicAccessBlock\",\n                \"s3:GetEncryptionConfiguration\",\n                \"s3:ListBucket\",\n                \"s3:ListBucketMultipartUploads\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::static.manselmi.com\"\n            ]\n        },\n        {\n            \"Sid\": \"ManselmiAllowS3ObjectStaticManselmiCom\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:AbortMultipartUpload\",\n                \"s3:DeleteObject\",\n                \"s3:GetObject\",\n                \"s3:GetObjectAttributes\",\n                \"s3:ListMultipartUploadParts\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::static.manselmi.com/*\"\n            ]\n        },\n        {\n            \"Sid\": \"ManselmiAllowCloudfrontStaticManselmiCom\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"cloudfront:CreateInvalidation\",\n                \"cloudfront:GetInvalidation\",\n                \"cloudfront:ListInvalidations\"\n            ],\n            \"Resource\": [\n                \"arn:aws:cloudfront::123456789012:distribution/ABCDEFGHIJKLMN\"\n            ]\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create IAM role <code>static.manselmi.com</code> and attach to it the previous permission policy. Edit the    role's trust policy so that any user within my AWS account may assume the role, but require    that the role's session name equal the username of the user assuming the role, and also require    multi-factor authentication:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789012:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"sts:RoleSessionName\": \"${aws:username}\"\n                },\n                \"Bool\": {\n                    \"aws:MultiFactorAuthPresent\": \"true\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create IAM permission policy <code>assume-role-static.manselmi.com</code> that allows assuming the previous    role:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ManselmiAllowAssumeRoleStaticManselmiCom\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::123456789012:role/static.manselmi.com\"\n            ]\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create IAM permission policy <code>credential-rotation</code> that allows the required IAM actions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"ManselmiAllowCredentialRotation\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:CreateAccessKey\",\n                \"iam:DeleteAccessKey\",\n                \"iam:GetUser\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::*:user/${aws:username}\"\n            ]\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Attach IAM permission policies <code>assume-role-static.manselmi.com</code> and <code>credential-rotation</code> to the    IAM user group <code>work</code>.</p> </li> <li> <p>Generate long-term credentials for IAM user <code>manselmi-work-sts</code> and assign to the user one or    more MFA devices.</p> </li> <li> <p>Add long-term credentials to aws-vault's storage backend:</p> <pre><code>aws-vault add manselmi\n</code></pre> </li> <li> <p>Update file <code>~/.aws/config</code>:</p> <pre><code>[profile manselmi]\ncredential_process = /Users/manselmi/.prefix/sw/homebrew/bin/aws-vault export --format=json -- manselmi\nregion = us-east-1\n\n[profile static.manselmi.com]\nduration_seconds = 1800\nmfa_process = /Users/manselmi/.prefix/sw/homebrew/bin/op read --no-newline op://&lt;vault-name&gt;/&lt;item-name&gt;/[section-name/]&lt;field-name&gt;?attribute=otp\nmfa_serial = arn:aws:iam::123456789012:mfa/1password-work\nregion = us-east-1\nrole_arn = arn:aws:iam::123456789012:role/static.manselmi.com\nrole_session_name = manselmi-work-sts\nsource_profile = manselmi\n</code></pre> </li> </ol>","tags":["aws","security"]},{"location":"aws-vault/#example-usage","title":"Example usage","text":"<pre><code>aws-vault exec static.manselmi.com -- aws s3 ls s3://static.manselmi.com/security/ --recursive\n</code></pre> <pre><code>2023-10-19 11:48:04       4369 security/gnupg/B397CE8214A7D51C1349384B140ABA5A34E6AB23.pub\n2023-10-16 23:53:53       1870 security/index.html\n2023-01-28 22:43:43        131 security/style.css\n</code></pre> <pre><code>aws-vault list\n</code></pre> <pre><code>Profile                  Credentials              Sessions\n=======                  ===========              ========\nmanselmi                 manselmi                 -\nstatic.manselmi.com      -                        sts.AssumeRole:11m35s\n</code></pre> <pre><code>DISTRIBUTION_ID='ABCDEFGHIJKLMN'\nPROFILE='static.manselmi.com'\n\naws-vault exec \"${PROFILE}\" -- aws cloudfront create-invalidation \\\n  --distribution-id \"${DISTRIBUTION_ID}\" \\\n  --paths '/foo' '/bar/*'\n</code></pre> <pre><code>{\n    \"Location\": \"https://cloudfront.amazonaws.com/2020-05-31/distribution/ABCDEFGHIJKLMN/invalidation/I1PPLLBTK6DB2HF1HJMQEYO5CC\",\n    \"Invalidation\": {\n        \"Id\": \"I1PPLLBTK6DB2HF1HJMQEYO5CC\",\n        \"Status\": \"InProgress\",\n        \"CreateTime\": \"2023-06-26T20:22:17.518000+00:00\",\n        \"InvalidationBatch\": {\n            \"Paths\": {\n                \"Quantity\": 2,\n                \"Items\": [\n                    \"/foo\",\n                    \"/bar/*\"\n                ]\n            },\n            \"CallerReference\": \"cli-1687810937-602265\"\n        }\n    }\n}\n</code></pre> <pre><code>INVALIDATION_ID='I1PPLLBTK6DB2HF1HJMQEYO5CC'\n\naws-vault exec \"${PROFILE}\" -- aws cloudfront get-invalidation \\\n  --distribution-id \"${DISTRIBUTION_ID}\" \\\n  --id \"${INVALIDATION_ID}\"\n</code></pre> <pre><code>{\n    \"Invalidation\": {\n        \"Id\": \"I1PPLLBTK6DB2HF1HJMQEYO5CC\",\n        \"Status\": \"Completed\",\n        \"CreateTime\": \"2023-06-26T20:22:17.518000+00:00\",\n        \"InvalidationBatch\": {\n            \"Paths\": {\n                \"Quantity\": 2,\n                \"Items\": [\n                    \"/foo\",\n                    \"/bar/*\"\n                ]\n            },\n            \"CallerReference\": \"cli-1687810937-602265\"\n        }\n    }\n}\n</code></pre> <pre><code># OOPS I just went out of my way to expose my long-term credentials!\naws-vault export --no-session -- manselmi\n</code></pre> <pre><code>AWS_ACCESS_KEY_ID=AKIA\u2026\nAWS_SECRET_ACCESS_KEY=QB7h\u2026\nAWS_REGION=us-east-1\nAWS_DEFAULT_REGION=us-east-1\n</code></pre> <pre><code># Let's rotate them immediately\u2026\naws-vault rotate --no-session -- manselmi\n</code></pre> <pre><code>Rotating credentials stored for profile 'manselmi' using master credentials (takes 10-20 seconds)\nCreating a new access key\nCreated new access key ****************LJ5Q\nDeleting old access key ****************BTUC\nDeleted old access key ****************BTUC\nFinished rotating access key\n</code></pre>","tags":["aws","security"]},{"location":"backup/","title":"Automated backup","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#preface","title":"Preface","text":"<p>This backup solution may appear to be a Rube Goldberg machine, and perhaps it is, but it works well for me and my family. I manage it headlessly over SSH (see tar over SSH). Thanks to <code>fdautil</code> I can modify things without needing to authorize Full Disk Access via the System Settings GUI.</p> <p>Recently I've also begun backing up my Ubuntu VPS with this approach as well.</p> <p>Also, if restic is good enough for CERN, it's good enough for me!</p> <ul> <li> <p>Empowering CERNBox users with self-service restore   functionality</p> </li> <li> <p>Addressing a billion-entries multi-petabyte distributed filesystem backup problem with cback:   from files to objects</p> </li> <li> <p>Backing up CERNBox: Lessons learned.</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#sequence-diagram","title":"Sequence diagram","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#macos","title":"macOS","text":"<pre><code>sequenceDiagram\n\n  participant KC as macOS \"login\" keychain\n  participant 1P as 1Password Connect\n\n  participant LD as launchd\n  participant ER as exec-resticprofile\n  participant RP as resticprofile\n  participant RE as restic\n  participant RC as rclone\n\n  participant PC as pCloud\n  participant HC as Healthchecks.io\n\n  LD -&gt;&gt; ER: Execute\n  ER -&gt;&gt; KC: Request 1Password Connect token\n  KC -&gt;&gt; ER: Receive 1Password Connect token\n  ER -&gt;&gt; RP: Execute w/ 1P Connect token in env\n  RP -&gt;&gt; 1P: Request healthcheck endpoint UUID\n  1P -&gt;&gt; RP: Receive healthcheck endpoint UUID\n  RP -&gt;&gt; HC: Ping healthcheck endpoint\n  RP -&gt;&gt; RE: Execute\n  RE -&gt;&gt; 1P: Request restic repo password\n  1P -&gt;&gt; RE: Receive restic repo password\n  RE -&gt;&gt; RC: Start HTTP server\n  RC -&gt;&gt; 1P: Request rclone config password\n  1P -&gt;&gt; RC: Receive rclone config password\n  RC -&gt;&gt; RE: HTTP connection established\n\n  loop Data transfer: backup, forget old snapshots\n    RE -&gt;&gt; RC: HTTP\n    RC -&gt;&gt; PC: HTTPS\n    PC -&gt;&gt; RC: HTTPS\n    RC -&gt;&gt; RE: HTTP\n  end\n\n  RE -&gt;&gt; RP: Return status\n  RP -&gt;&gt; 1P: Request healthcheck endpoint UUID\n  1P -&gt;&gt; RP: Receive healthcheck endpoint UUID\n  RP -&gt;&gt; HC: Ping healthcheck endpoint w/ status and log file\n</code></pre> <p>The diagram may also be viewed with interactive controls here.</p> <p>Note</p> <p>We can assume the default \"login\" keychain is unlocked because</p> <ol> <li> <p>the launchd service is running in the logged-in user's launchd GUI domain, and</p> </li> <li> <p>the \"login\" keychain automatically unlocks upon login and has been configured to remain    unlocked while logged in.</p> </li> </ol> <p>This allows the 1Password Connect token to be loaded from the \"login\" keychain with no user interaction required.</p> <p></p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#ubuntu-vps","title":"Ubuntu (VPS)","text":"<pre><code>sequenceDiagram\n\n  participant FS as Local filesystem\n  participant 1P as 1Password\n\n  participant SD as systemd\n  participant ER as exec-resticprofile\n  participant RP as resticprofile\n  participant RE as restic\n  participant RC as rclone\n\n  participant PC as pCloud\n  participant HC as Healthchecks.io\n\n  SD -&gt;&gt; ER: Execute\n  ER -&gt;&gt; FS: Request 1Password service account token\n  FS -&gt;&gt; ER: Receive 1Password service account token\n  ER -&gt;&gt; RP: Execute w/ 1P svc acct token in env\n  RP -&gt;&gt; 1P: Request healthcheck endpoint UUID\n  1P -&gt;&gt; RP: Receive healthcheck endpoint UUID\n  RP -&gt;&gt; HC: Ping healthcheck endpoint\n  RP -&gt;&gt; RE: Execute\n  RE -&gt;&gt; 1P: Request restic repo password\n  1P -&gt;&gt; RE: Receive restic repo password\n  RE -&gt;&gt; RC: Start HTTP server\n  RC -&gt;&gt; 1P: Request rclone config password\n  1P -&gt;&gt; RC: Receive rclone config password\n  RC -&gt;&gt; RE: HTTP connection established\n\n  loop Data transfer: backup, forget old snapshots\n    RE -&gt;&gt; RC: HTTP\n    RC -&gt;&gt; PC: HTTPS\n    PC -&gt;&gt; RC: HTTPS\n    RC -&gt;&gt; RE: HTTP\n  end\n\n  RE -&gt;&gt; RP: Return status\n  RP -&gt;&gt; 1P: Request healthcheck endpoint UUID\n  1P -&gt;&gt; RP: Receive healthcheck endpoint UUID\n  RP -&gt;&gt; HC: Ping healthcheck endpoint w/ status and log file\n</code></pre> <p>The diagram may also be viewed with interactive controls here.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#examples","title":"Examples","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#macos-ubuntu-vps","title":"macOS / Ubuntu (VPS)","text":"<pre><code># Manually start backup and tail the backup log.\nbk\n\n# Log current backup progress.\nbkp\n</code></pre>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#google-drive","title":"Google Drive","text":"<pre><code># Mount Google Drive locally (runs in foreground - interrupt when backup finished)\nmnt-gdrive\n\n# Start Google Drive backup and tail the backup log.\nbk-gdrive\n\n# Log current backup progress.\nbkp\n</code></pre>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#configuration-support-files","title":"Configuration / support files","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#macos_1","title":"macOS","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#launchd","title":"launchd","text":"<ul> <li> <p><code>~/Library/LaunchAgents/com.manselmi.resticprofile.nidoking.backup.plist</code>   (triggered daily)</p> </li> <li> <p><code>~/Library/LaunchAgents/com.manselmi.resticprofile.gdrive.backup.plist</code>   (triggered manually)</p> </li> <li> <p><code>~/Library/LaunchAgents/com.manselmi.resticprofile.onedrive.backup.plist</code>   (triggered manually)</p> </li> <li> <p><code>~/Library/LaunchAgents/com.manselmi.op.connect.server.plist</code>   (run at load)</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#fdautil","title":"fdautil","text":"<ul> <li><code>/Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist</code></li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#resticprofile","title":"resticprofile","text":"<ul> <li> <p><code>~/.prefix/bin/exec-resticprofile</code></p> </li> <li> <p><code>~/.config/resticprofile/profiles.toml</code></p> </li> <li> <p><code>~/.config/resticprofile/</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#sudo","title":"Sudo","text":"<ul> <li><code>/private/etc/sudoers.d/</code></li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#rclone","title":"Rclone","text":"<ul> <li> <p><code>~/.prefix/bin/exec-rclone</code></p> </li> <li> <p><code>~/.config/rclone/rclone.conf</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#1password","title":"1Password","text":"<ul> <li> <p><code>~/.prefix/bin/exec-op-token</code></p> </li> <li> <p><code>~/.prefix/bin/compose-wrapper</code></p> </li> <li> <p><code>~/.config/op-connect/compose.yaml</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#zsh","title":"Zsh","text":"<ul> <li><code>~/.zshrc</code></li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#ubuntu-vps_1","title":"Ubuntu (VPS)","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#systemd","title":"systemd","text":"<ul> <li> <p><code>/etc/systemd/system/resticprofile-backup@.service</code></p> </li> <li> <p><code>/etc/systemd/system/resticprofile-backup@.timer</code>   (triggered daily)</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#resticprofile_1","title":"resticprofile","text":"<ul> <li> <p><code>~/.prefix/bin/exec-resticprofile</code></p> </li> <li> <p><code>~/.config/resticprofile/profiles.toml</code></p> </li> <li> <p><code>~/.config/resticprofile/</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#rclone_1","title":"Rclone","text":"<ul> <li> <p><code>~/.prefix/bin/exec-rclone</code></p> </li> <li> <p><code>~/.config/rclone/rclone.conf</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#1password_1","title":"1Password","text":"<ul> <li><code>~/.prefix/bin/exec-op-token</code></li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#zsh_1","title":"Zsh","text":"<ul> <li><code>~/.zshrc</code></li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#components","title":"Components","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#restic","title":"restic","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url","title":"Project URL","text":"<p>restic</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description","title":"Project description","text":"<p>Restic is a modern backup program that can back up your files:</p> <ul> <li> <p>from Linux, BSD, Mac and Windows</p> </li> <li> <p>to many different storage types, including self-hosted and online services</p> </li> <li> <p>easily, being a single executable that you can run without a server or complex setup</p> </li> <li> <p>effectively, only transferring the parts that actually changed in the files you back up</p> </li> <li> <p>securely, by careful use of cryptography in every part of the process</p> </li> <li> <p>verifiably, enabling you to make sure that your files can be restored when needed</p> </li> <li> <p>freely - restic is entirely free to use and completely open source</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#resticprofile_2","title":"resticprofile","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_1","title":"Project URL","text":"<p>resticprofile</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_1","title":"Project description","text":"<p>Configuration profiles manager for restic backup</p> <p>resticprofile is the missing link between a configuration file and restic backup. Creating a configuration file for restic has been discussed before, but seems to be a very low priority right now.</p> <p>With resticprofile:</p> <ul> <li> <p>You no longer need to remember command parameters and environment variables</p> </li> <li> <p>You can create multiple profiles inside one configuration file</p> </li> <li> <p>A profile can inherit all the options from another profile</p> </li> <li> <p>You can run the forget command before or after a backup (in a section called retention)</p> </li> <li> <p>You can check a repository before or after a backup</p> </li> <li> <p>You can create groups of profiles that will run sequentially</p> </li> <li> <p>You can run shell   commands before or   after running a profile: useful if you need to mount and unmount your backup disk for example</p> </li> <li> <p>You can run a shell   command if an error   occurred (at any time)</p> </li> <li> <p>You can send a backup stream via stdin</p> </li> <li> <p>You can start restic at a lower or higher priority (Priority Class in Windows, nice in all   unixes) and/or ionice (only available on Linux)</p> </li> <li> <p>It can check that you have enough   memory before starting a backup.   (I've had some backups that literally killed a server with swap disabled)</p> </li> <li> <p>You can generate cryptographically secure random keys to use as a restic key   file</p> </li> <li> <p>You can easily schedule backups,   retentions and checks (works for systemd, crond, launchd and windows task scheduler)</p> </li> <li> <p>You can generate a simple status file   to send to some monitoring software and make sure your backups are running fine</p> </li> <li> <p>You can use a template syntax in your configuration file</p> </li> <li> <p>You can generate scheduled tasks using crond</p> </li> <li> <p>Get backup statistics in your status   file</p> </li> <li> <p>Automatically clear up stale   locks</p> </li> <li> <p>Export a prometheus file   after a backup, or send the report to a push gateway automatically</p> </li> <li> <p>Run shell commands in the background when non fatal errors are detected from restic</p> </li> <li> <p>Send messages to HTTP   hooks before, after a   successful or failed job (backup, forget, check, prune, copy)</p> </li> <li> <p>Automatically initialize the secondary repository using <code>copy-chunker-params</code> flag</p> </li> <li> <p>Send resticprofile logs to a syslog server</p> </li> <li> <p>Preventing your system from idle sleeping</p> </li> <li> <p>See the help from both restic and resticprofile via the <code>help</code> command or <code>-h</code> flag</p> </li> <li> <p>Don't schedule a job when the system is running on battery</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#rclone_2","title":"rclone","text":"<p>Please see rclone.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#fuse-t","title":"FUSE-T","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_2","title":"Project URL","text":"<p>FUSE-T</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_2","title":"Project description","text":"<p>FUSE-T is a kext-less implementation of FUSE for macOS that uses NFS v4 local server instead of a kernel extension.</p> <p>The main motivation for this project is to replace macfuse that implements its own kext to make fuse work. With each version of macOS it's getting harder and harder to load kernel extensions. Apple strongly discourages it and, for this reason, software distributions that include macfuse are very difficult to install.</p> <p>Additionally, the macfuse kext is unstable, may cause frequent system crashes and kernel lock-ups. Given those limitations many software publishers are unable to use macfuse anymore.</p> <p>FUSE-T doesn't make use of kernel extension, it implements its own userspace server that converts between FUSE protocol and NFS calls and let macOS mount NFS volume instead of a kernel filesystem.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#healthchecksio","title":"Healthchecks.io","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_3","title":"Project URL","text":"<p>Healthchecks.io</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_3","title":"Project description","text":"<p>Healthchecks.io is an online service for monitoring regularly running tasks such as cron jobs. It uses the Dead man's switch technique: the monitored system must \"check in\" with Healthchecks.io at regular, configurable time intervals. When Healthchecks.io detects a missed check-in, it sends out alerts.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#1password-cli","title":"1Password CLI","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_4","title":"Project URL","text":"<ul> <li> <p>1Password CLI</p> </li> <li> <p><code>op inject</code></p> </li> <li> <p><code>op read</code></p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_4","title":"Project description","text":"<p>1Password CLI allows you to securely provision secrets in development environments, use scripts to manage items and provision team members at scale, and authenticate with biometrics in the terminal.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#1password-connect","title":"1Password Connect","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_5","title":"Project URL","text":"<ul> <li> <p>1Password Connect</p> </li> <li> <p>Use 1Password CLI with a Connect server</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_5","title":"Project description","text":"<p>1Password Connect servers are a type of Secrets Automation workflow that allows you to securely access your 1Password items and vaults in your company's apps and cloud infrastructure.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#launchcontrol-fdautil","title":"LaunchControl / fdautil","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_6","title":"Project URL","text":"<p>LaunchControl</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_6","title":"Project description","text":"<p>LaunchControl is a fully-featured launchd GUI allowing you to create, manage and debug system- and user services on your Mac.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#full-disk-access","title":"Full Disk Access","text":"<p>(This section is copied from the LaunchControl manual.)</p> <p>With macOS Mojave Apple added yet another security measure. Applications requiring access to sensitive data need special permissions beyond Unix permissions and ACLs. This is accomplished by granting them Full Disk Access in the Security &amp; Privacy System Preferences Panel. While this solution works well for applications it is flawed when it comes to scripts. To grant a script (be it Shell, Python, Perl, \u2026) Full Disk Access you have grant Full Disk Access to the executing interpreter. This approach is flawed as it grants every script of this type Full Disk Access.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#fdautil_1","title":"fdautil","text":"<p>LaunchControl provides a command line utility to avoid this situation. The utility is called <code>fdautil</code> and can be installed via the LaunchControl preferences Utilities panel to <code>/usr/local/bin</code>. When installing <code>fdautil</code> LaunchControl will ask you to add it to the list of programs with Full Disk Access.</p> <p>To grant a job Full Disk Access you\u2019ll have to prefix the command line you\u2019d like to execute with <code>/usr/local/bin/fdautil exec</code>. If for example your original job performed the command</p> <pre><code>/Users/Me/Scripts/CheckDownloads.py\n</code></pre> <p>you change it to</p> <pre><code>/usr/local/bin/fdautil exec /Users/Me/Scripts/CheckDownloads.py\n</code></pre> <p>LaunchControl requires admin privileges to add your script (including the specific options and arguments) to the fdautil configuration file.</p> <p>Note: On macOS Ventura services which run with fdautil are displayed as \"Robby Pahlig\" in System Settings \u2192 General \u2192 Login Items \u2192 Allow in the Background. Apple chose to display the name of the signing entity of an executable instead of a more informative text.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#how-does-it-work","title":"How does it work?","text":"<p><code>fdautil</code> stores the complete command you want to execute with Full Disk Access in the configuration file <code>/Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist</code>. This configuration file is writable only by root. Only commands (including arguments/options) stored in this configuration file are executed by fdautil, others are rejected. It uses <code>execvp(3)</code> to replace itself with the command you\u2019d like to run. LaunchControl does all the configuration for you. When you change the <code>Program</code>/<code>ProgramArguments</code> key it is updating the fdautil configuration automatically.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#what-is-gained","title":"What is gained?","text":"<p>You get the benefit of Apple\u2019s new security feature and can use scripts with Full Disk Access.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#launchd_1","title":"launchd","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_7","title":"Project URL","text":"<ul> <li> <p>Daemons and Services Programming   Guide</p> </li> <li> <p>A launchd Tutorial</p> </li> </ul>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_7","title":"Project description","text":"<p>launchd is an init and operating system service management daemon created by Apple Inc. as part of macOS to replace its BSD-style init and SystemStarter.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#systemd_1","title":"systemd","text":"","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-url_8","title":"Project URL","text":"<p>The systemd System and Service Manager</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"backup/#project-description_8","title":"Project description","text":"<p>systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system.</p> <p>systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, maintains mount and automount points, and implements an elaborate transactional dependency-based service control logic. systemd supports SysV and LSB init scripts and works as a replacement for sysvinit.</p> <p>Other parts include a logging daemon, utilities to control basic system configuration like the hostname, date, locale, maintain a list of logged-in users and running containers and virtual machines, system accounts, runtime directories and settings, and daemons to manage simple network configuration, network time synchronization, log forwarding, and name resolution.</p>","tags":["launchd","mermaid","rclone","security"]},{"location":"devpi/","title":"devpi","text":"","tags":["launchd","mermaid","python"]},{"location":"devpi/#project-url","title":"Project URL","text":"<p>devpi</p>","tags":["launchd","mermaid","python"]},{"location":"devpi/#introduction","title":"Introduction","text":"<p>devpi lets you host one or more Python package indexes locally. The following diagram and examples demonstrate how pip will work after following the instructions on this page.</p> <pre><code>---\nconfig:\n  layout: elk\n---\n\nflowchart TB\n\n  subgraph internet [\"Internet\"]\n    pypi-pub[(\"PyPI&lt;br&gt;(Python Package Index)\")]\n  end\n\n  subgraph intranet [\"Intranet\"]\n    internal-art[(\"python-local&lt;br&gt;(Artifactory repo)\")]\n  end\n\n  subgraph devpi [\"devpi\"]\n    pypi-devpi[\\\"pypi&lt;br&gt;(pass-through)\"/]\n    internal-devpi[(\"python-local&lt;br&gt;(cache)\")]\n    local[(\"local\")]\n\n    pypi-devpi &amp; internal-devpi --&gt; local\n  end\n\n  pypi-pub --&gt; pypi-devpi\n  internal-art --&gt; internal-devpi\n\n  local --&gt; pip</code></pre> <p>Two examples of how this will work:</p> <ul> <li> <p><code>pip install -- numpy</code></p> <ol> <li> <p>pip searches devpi's <code>local</code> index for a package named <code>numpy</code>.</p> </li> <li> <p>devpi doesn't find <code>numpy</code> in the <code>local</code> devpi index, so devpi searches the <code>pypi</code> and    <code>python-local</code> devpi indexes.</p> </li> <li> <p>The <code>pypi</code> devpi index searches the PyPI index on the Internet. <code>numpy</code> is found but is not    cached locally.</p> </li> <li> <p>The <code>python-local</code> devpi index searches the <code>python-local</code> Artifactory index within the    intranet. <code>numpy</code> is not found.</p> <ul> <li>Even if the intranet is not available, the result is the same: <code>numpy</code> is not found.</li> </ul> </li> <li> <p>The <code>numpy</code> found via the <code>pypi</code> devpi index has a greater version string than the <code>numpy</code>    found via the <code>python-local</code> devpi index (vacuously true since the <code>python-local</code>    devpi index could not find any version of <code>numpy</code>), so the <code>numpy</code> found via the <code>pypi</code>    devpi index is served to the <code>local</code> devpi index.</p> </li> <li> <p>devpi's <code>local</code> index serves <code>numpy</code> to pip.</p> </li> </ol> </li> <li> <p><code>pip install -- private-package</code></p> <ol> <li> <p>pip searches devpi's <code>local</code> index for a package named <code>private-package</code>.</p> </li> <li> <p>devpi doesn't find <code>private-package</code> in the <code>local</code> devpi index, so devpi searches the <code>pypi</code>    and <code>python-local</code> devpi indexes.</p> </li> <li> <p>The <code>pypi</code> devpi index searches the the PyPI index on the Internet. <code>private-package</code> is not    found.</p> </li> <li> <p>The <code>python-local</code> devpi index searches the <code>python-local</code> Artifactory index within the    intranet.</p> <ul> <li> <p>If the intranet is available, the latest version of <code>private-package</code> is found and is   cached locally.</p> </li> <li> <p>If the intranet is not available, a cached copy of the latest version of <code>private-package</code>   previously downloaded is found.</p> <ul> <li>If <code>private-package</code> has never been downloaded before, then <code>private-package</code> is not   found and we stop here, with pip reporting that <code>private-package</code> could not be found.</li> </ul> </li> </ul> </li> <li> <p>The <code>private-package</code> found via the <code>python-local</code> devpi index has a greater version string    than the <code>private-package</code> found via the <code>pypi</code> devpi index (vacuously true since the <code>pypi</code>    devpi index could not find any version of <code>private-package</code>), so the <code>private-package</code>    found via the <code>python-local</code> devpi index is served to the <code>local</code> devpi index.</p> </li> <li> <p>devpi's <code>local</code> index serves <code>private-package</code> to pip.</p> </li> </ol> </li> </ul> <p>Please note that if both PyPI and the <code>python-local</code> Artifactory repo both have a package with the same name, then devpi will fetch the package with the greatest version string.</p> <p>For this reason, internal Python package names should have an org-specific prefix such as <code>foo.</code> to reduce the likelihood of name collisions with public Python package names. Please see my Python library template for a project template that enforces such a prefix.</p>","tags":["launchd","mermaid","python"]},{"location":"devpi/#installation-instructions","title":"Installation instructions","text":"<p>First, install Pixi. Please note that Pixi installation and configuration instructions are outside of the scope of this document.</p> <p>Create some directories we'll need:</p> <pre><code>mkdir -p -- \\\n  \"${HOME}/.devpi\" \\\n  \"${HOME}/.taskfile/devpi\" \\\n  \"${HOME}/Library/LaunchAgents\"\n</code></pre> <p>Place the Pixi manifest file in the <code>~/.taskfile/devpi</code> directory:</p> pixi.toml<pre><code># vim: set ft=toml :\n\n\n[project]\nname = 'devpi'\nchannels = ['conda-forge']\nplatforms = [\n  # 'osx-64',\n  'osx-arm64',\n]\n\n\n[dependencies]\ndevpi-client = '*'\ndevpi-server = '*'\n</code></pre> <p>Create an isolated environment in which we'll install devpi, then initialize devpi:</p> <pre><code>pushd -q -- \"${HOME}/.taskfile/devpi\"\npixi update\npixi install\npixi run -- devpi-init \\\n  --role standalone \\\n  --root-passwd root \\\n  --serverdir \"${HOME}/.devpi/server\" \\\n  --storage sqlite\npopd -q\n</code></pre> <p>Now let's create a launchd service that will make it easy to automatically start and stop devpi. Add this devpi launchd service definition to the <code>~/Library/LaunchAgents</code> directory, editing usernames and pathnames as needed:</p> net.devpi.server.plist<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!-- vim: set ft=xml : --&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n    &lt;key&gt;Disabled&lt;/key&gt;\n    &lt;false/&gt;\n    &lt;key&gt;EnvironmentVariables&lt;/key&gt;\n    &lt;dict&gt;\n        &lt;key&gt;PATH&lt;/key&gt;\n        &lt;string&gt;/Users/manselmi/.prefix/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin&lt;/string&gt;\n        &lt;key&gt;TZ&lt;/key&gt;\n        &lt;string&gt;UTC&lt;/string&gt;\n    &lt;/dict&gt;\n    &lt;key&gt;KeepAlive&lt;/key&gt;\n    &lt;dict&gt;\n        &lt;key&gt;SuccessfulExit&lt;/key&gt;\n        &lt;false/&gt;\n    &lt;/dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;net.devpi.server&lt;/string&gt;\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n    &lt;array&gt;\n        &lt;string&gt;/Users/manselmi/.devpi/devpi-server&lt;/string&gt;\n        &lt;string&gt;/Users/manselmi/.taskfile/devpi/.pixi/envs/default/bin/devpi-server&lt;/string&gt;\n        &lt;string&gt;--port&lt;/string&gt;\n        &lt;string&gt;3141&lt;/string&gt;\n        &lt;string&gt;--serverdir&lt;/string&gt;\n        &lt;string&gt;/Users/manselmi/.devpi/server&lt;/string&gt;\n    &lt;/array&gt;\n    &lt;/array&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre> <p>If you would like to learn more about launchd, please see Creating Launch Daemons and Agents.</p> <p>Second, let's create the <code>~/.devpi/devpi-server</code> file invoked by the launchd service, editing usernames and pathnames as needed:</p> devpi-server<pre><code>#!/Users/manselmi/.taskfile/devpi/.pixi/envs/default/bin/python\n# vim: set ft=python :\n\nimport signal\nimport subprocess\nimport sys\n\n\ndef main():\n    process = None\n\n    def handler(signum, frame):\n        if process is not None:\n            process.send_signal(signal.SIGINT)\n\n    signal.signal(signal.SIGINT, handler)\n    signal.signal(signal.SIGTERM, handler)\n\n    process = subprocess.Popen(sys.argv[1:], start_new_session=True)\n    process.wait()\n\n    sys.exit(process.returncode)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>The launchd service we created will run when loaded, so let's load the service:</p> <pre><code>launchctl bootstrap \"gui/$(id -u)/\" ~/Library/LaunchAgents/net.devpi.server.plist\n</code></pre> <p>Please note that upon future logins, the service will automatically be loaded and hence automatically started.</p> <p>Now let's confirm that devpi is up and running. Navigate your browser to http://localhost:3141. If you see a devpi page, then so far so good.</p>","tags":["launchd","mermaid","python"]},{"location":"devpi/#configuration-instructions","title":"Configuration instructions","text":"<p>In this section we'll configure devpi to behave as described in the introduction, and then we'll configure pip to use devpi.</p> <p>These commands configure devpi. I suggest running these specific commands one at a time.</p> <pre><code>pushd -q -- \"${HOME}/.taskfile/devpi\"\npixi shell\nunset DEVPI_INDEX\ndevpi use --always-set-cfg no http://localhost:3141/  # this might complain; no worries\ndevpi login --password root root\ndevpi index pypi mirror_use_external_urls=True  # don't cache PyPI packages\ndevpi index -c python-local \\\n  mirror_url='https://artifactory.example.com/artifactory/api/pypi/python-local/simple/' \\\n  title='Intranet: python-local' \\\n  type=mirror \\\n  volatile=False\ndevpi index -c local \\\n  bases='root/python-local,root/pypi' \\\n  title='Local: personal index'\ndevpi use --venv - root/local\nexit\npopd -q\n</code></pre> <p>Note</p> <p>If you would like to cache all packages (including those from PyPI), run the following command before exiting the Pixi environment shell:</p> <pre><code>devpi index pypi mirror_use_external_urls=False\n</code></pre> <p>However, be aware that devpi will consume more disk space.</p> <p>Configure pip, distutils, buildout etc to use devpi:</p> ~/Library/Application Support/pip/pip.conf<pre><code>[global]\nindex-url = http://localhost:3141/root/local/+simple/\ntrusted-host = localhost\n\n[search]\nindex = http://localhost:3141/root/local/\n</code></pre> ~/.pydistutils.cfg<pre><code>[easy_install]\nindex_url = http://localhost:3141/root/local/+simple/\n</code></pre> ~/.buildout/default.cfg<pre><code>[buildout]\nindex = http://localhost:3141/root/local/+simple/\n</code></pre>","tags":["launchd","mermaid","python"]},{"location":"devpi/#validation","title":"Validation","text":"<p>Let's confirm that pip and devpi are working as expected.</p> <p>First, let's try downloading <code>numpy</code>:</p> <pre><code>rm -f -- numpy-*.whl\npython -m pip --no-cache-dir download --no-deps --prefer-binary -- numpy\n# Looking in indexes: http://localhost:3141/root/local/+simple/\n# Collecting numpy\n#   Downloading http://localhost:3141/root/pypi/%2Bf/afd/5ced4e5a96dac/numpy-1.26.1-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n#      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.7/13.7 MB 14.8 MB/s eta 0:00:00\n# Saved ./numpy-1.26.1-cp312-cp312-macosx_11_0_arm64.whl\n# Successfully downloaded numpy\n</code></pre> <p>Finally, let's try downloading <code>private-package</code> while connected to the intranet:</p> <pre><code>rm -f -- private-package-*.tar.gz\npython -m pip --no-cache-dir download --no-deps -- private-package\n# Looking in indexes: http://localhost:3141/root/local/+simple/\n# Collecting private-package\n#   Downloading http://localhost:3141/root/python-local/%2Bf/77b/253cc0ae627fb/private-package-2.0.0.tar.gz (208 kB)\n#      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 208.8/208.8 kB 675.9 MB/s eta 0:00:00\n#   Preparing metadata (setup.py) ... done\n# Saved ./private-package-2.0.0.tar.gz\n# Successfully downloaded private-package\n</code></pre> <p>We're seeing a fast download speed because my <code>devpi</code> instance already has the latest version of <code>private-package</code> in its cache. devpi checked with Artifactory and determined that I already had the latest version cached, so devpi served the cached copy\u2026 very quickly.</p> <p>Now let's try while not connected to the intranet.</p> <pre><code>rm -f -- private-package-*.tar.gz\npython -m pip --no-cache-dir download --no-deps -- private-package\n# Looking in indexes: http://localhost:3141/root/local/+simple/\n# Collecting private-package\n#   Downloading http://localhost:3141/root/python-local/%2Bf/77b/253cc0ae627fb/private-package-2.0.0.tar.gz (208 kB)\n#      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 208.8/208.8 kB 573.3 MB/s eta 0:00:00\n#   Preparing metadata (setup.py) ... done\n# Saved ./private-package-2.0.0.tar.gz\n# Successfully downloaded private-package\n</code></pre> <p>Same result.</p>","tags":["launchd","mermaid","python"]},{"location":"devpi/#rancher-desktop","title":"Rancher Desktop","text":"<p>A nice bonus is that we can also instruct pip within a container to leverage devpi. This makes it easier to work with Python within a container regardless of whether or not we're connected to the intranet.</p> <p>Ensure you're not connected to the intranet before running the following command:</p> <pre><code>nerdctl container run \\\n  --env PIP_INDEX_URL=http://host.lima.internal:3141/root/local/+simple/ \\\n  --env PIP_TRUSTED_HOST=host.lima.internal \\\n  --rm \\\n  -- \\\n  docker.io/library/python:3.12.0-bookworm \\\n  pip download --no-deps -- private-package numpy\n# Looking in indexes: http://host.lima.internal:3141/root/local/+simple/\n# Collecting private-package\n#   Downloading http://host.lima.internal:3141/root/python-local/%2Bf/77b/253cc0ae627fb/private-package-2.0.0.tar.gz (208 kB)\n#      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 208.8/208.8 kB 19.1 MB/s eta 0:00:00\n#   Preparing metadata (setup.py): started\n#   Preparing metadata (setup.py): finished with status 'done'\n# Collecting numpy\n#   Downloading http://host.lima.internal:3141/root/pypi/%2Bf/a03/fb25610ef560a/numpy-1.26.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (13.9 MB)\n#      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.9/13.9 MB 15.4 MB/s eta 0:00:00\n# Saved /private-package-2.0.0.tar.gz\n# Saved /numpy-1.26.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# Successfully downloaded private-package numpy\n</code></pre>","tags":["launchd","mermaid","python"]},{"location":"devpi/#maintenance","title":"Maintenance","text":"<p>To stop the devpi service:</p> <pre><code>launchctl bootout \"gui/$(id -u)/net.devpi.server\"\n</code></pre> <p>To start the devpi service (if not already running):</p> <pre><code>launchctl bootstrap \"gui/$(id -u)/\" ~/Library/LaunchAgents/net.devpi.server.plist\n</code></pre> <p>To upgrade devpi:</p> <pre><code>launchctl bootout \"gui/$(id -u)/net.devpi.server\"\npushd -q -- \"${HOME}/.taskfile/devpi\"\npixi update\npixi install\npopd -q\nlaunchctl bootstrap \"gui/$(id -u)/\" ~/Library/LaunchAgents/net.devpi.server.plist\n</code></pre> <p>To uninstall devpi, run:</p> <pre><code>launchctl bootout \"gui/$(id -u)/net.devpi.server\"\nrm -r -- \\\n  \"${HOME}/.buildout\" \\\n  \"${HOME}/.devpi\" \\\n  \"${HOME}/.pydistutils.cfg\" \\\n  \"${HOME}/Library/Application Support/pip/pip.conf\" \\\n  \"${HOME}/Library/LaunchAgents/net.devpi.server.plist\"\npushd -q -- \"${HOME}/.taskfile/devpi\"\npixi clean\npopd -q\n</code></pre>","tags":["launchd","mermaid","python"]},{"location":"git/","title":"Git","text":"","tags":["git","shell"]},{"location":"git/#configuration","title":"Configuration","text":"","tags":["git","shell"]},{"location":"git/#global","title":"Global","text":"<ul> <li><code>~/.gitconfig</code></li> </ul>","tags":["git","shell"]},{"location":"git/#tips-tricks","title":"Tips &amp; tricks","text":"","tags":["git","shell"]},{"location":"git/#identify-distinct-file-extensions","title":"Identify distinct file extensions","text":"<p>The following command pipeline prints all distinct file extensions, or filenames if the filename has no extension:</p> <pre><code>find -- . -type d -name .git -prune -o \\! -type d -print0 \\\n  | gawk -v RS='\\0' -v ORS='\\0' -- '{ match($0, /\\/([^\\/]+)$/, a); s = a[1]; print match(s, /(\\.[^.]+)$/, a) ? a[1] : s }' \\\n  | sort -fuz \\\n  | gawk -v RS='\\0' -v ORS='\\n' -- '{ print gensub(/[^[:print:]]+/, \"\ufffd\", \"g\") }'\n</code></pre> <p>This helps with creating gitignore and gitattributes files.</p>","tags":["git","shell"]},{"location":"git/#debug-gitignore-patterns","title":"Debug gitignore patterns","text":"<p>The following command pipeline prints a four-column table that may be useful for debugging gitignore patterns:</p> <pre><code>find -- . -type d -name .git -prune -o \\! -type d -print0 \\\n  | sort -fz \\\n  | git check-ignore -nvz --no-index --stdin \\\n  | gawk -v RS='\\0' -v ORS='\\n' -- '{ print gensub(/[^[:print:]]+/, \"\ufffd\", \"g\") }' \\\n  | paste -s -d '\\t\\t\\t\\n' -- - \\\n  | awk -v FS='\\t' -v OFS='\\t' -- '{ print $4, $1, $2, $3 }' \\\n  | column -t -s $'\\t'\n</code></pre> <p>The four columns:</p> <ol> <li>path of a file being queried</li> <li>pattern's source file</li> <li>line number of the pattern within that source</li> <li>matching pattern</li> </ol> <p>If some file does not match a gitignore pattern, then all columns except (1) will be empty.</p>","tags":["git","shell"]},{"location":"git/#debug-text-and-eol-attributes","title":"Debug <code>text</code> and <code>eol</code> attributes","text":"<p>The following command pipeline prints a four-column table that may be helpful for debugging the <code>text</code> and <code>eol</code> attributes. Among other things, it prints the file content identification used by Git when the <code>text</code> attribute is <code>auto</code> (or not set and the <code>core.autocrlf</code> config option is not false).</p> <pre><code>read -r -d '' GAWK_PROG &lt;&lt; 'EOF' || true\nBEGIN {\n  RS = \"\\0\"\n  OFS = \"\\t\"\n  ORS = \"\\n\"\n}\n{\n  match($0, /^([^ ]+) +([^ ]+) +([^\\t]+)\\t(.+)$/, a)\n  a[3] = gensub(/ +$/, \"\", \"g\", a[3])\n  a[4] = gensub(/[^[:print:]]+/, \"\ufffd\", \"g\", a[4])\n  print a[1], a[2], a[3], a[4]\n}\nEOF\n\ngit ls-files -coz --eol --exclude-standard \\\n  | gawk -- \"${GAWK_PROG}\" \\\n  | sort -f -t $'\\t' -k 1,1 -k 2,2 -k 3,3 -k 4,4 \\\n  | column -t -s $'\\t'\n</code></pre> <p>The four columns:</p> <ol> <li> <p><code>eolinfo</code> of the contents in the index for the path. Possible values:</p> <pre><code>i/\ni/-text\ni/crlf\ni/lf\ni/mixed\ni/none\n</code></pre> </li> <li> <p><code>eolinfo</code> of the contents in the worktree for the path. Possible values:</p> <pre><code>w/\nw/-text\nw/crlf\nw/lf\nw/mixed\nw/none\n</code></pre> </li> <li> <p><code>eolattr</code> that applies to the path. Possible values:</p> <pre><code>eolattr/\neolattr/-text\neolattr/text\neolattr/text eol=crlf\neolattr/text eol=lf\neolattr/text=auto\neolattr/text=auto eol=crlf\neolattr/text=auto eol=lf\n</code></pre> </li> <li> <p>path</p> </li> </ol>","tags":["git","shell"]},{"location":"git/#renormalize-repository","title":"Renormalize repository","text":"<p>After modifying a gitattributes file, run the following from a clean working directory:</p> <pre><code>git add --renormalize -- .\ngit commit -m 'git add --renormalize -- .'\n</code></pre>","tags":["git","shell"]},{"location":"gnupg-key-management/","title":"GnuPG Key Management","text":"<p>Original title: \"Proper Key Management\"</p> <p>permalink by Saklad5</p> <p>I've been using GnuPG for years now, and I've noticed that a lot of people misunderstand or overlook the intent of many features. People encounter the same thorny problems over and over, not realizing they've already been solved to some degree.</p> <p>With that in mind, I thought I'd share my own advice on the ideal way to use GnuPG (and OpenPGP in general). If you don't feel like reading through all of it, skip to the bottom for the most important feature no one uses.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#user-ids","title":"User IDs","text":"<p>Each UID is tied directly to a primary key, on a many-to-one basis. The same is true for subkeys. This means that every subkey is tied to every UID, and vice versa.</p> <p>If you want subkeys that are only tied to specific UIDs, you should make a new primary key. For instance, if you want to sign your Git commits on your work computer, you should make a new primary key with your work email as a UID, then use signing subkeys of that. That way, the signing keys can only be used in the name of your work identity, and your other signing keys cannot be used to do the same.</p> <p>Using multiple UIDs on the same primary key is only reasonable if you use all of them in the same capacity, or otherwise want to share subkeys between them.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#subkeys","title":"Subkeys","text":"<p>PGP keys have four functions. Two are fungible: signing and authenticating. This means that the specific key used to perform them isn't meaningful, and they can be used interchangeably so long as they share the same primary key. This means you can (and should) generate a new subkey for the purpose on each device you have, and avoid having multiple copies of them at all. When you stop using an old device, revoke these keys accordingly. Don't even worry about losing access to them: they are trivially replaceable.</p> <p>The other two functions, certifying and encrypting, are not fungible. The exact key you use is critical, and a different subkey will not work the same. This makes them much more important, and much more sensitive. Transitioning between them is painful and best avoided. They often have to be shared across devices, and should be locked with unique passwords to stop mistakes or malicious attacks.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#omit-needless-keys","title":"Omit Needless Keys","text":"<p>The default behavior of GnuPG generates a primary key capable of certifying and signing, and a subkey capable of encryption. I'm here to tell you that is stupid: you should never be signing anything with your precious certifying primary key when you could be using a fungible signing-only subkey, and you shouldn't generate encryption keys until you actually have a use for them in mind: it's not worth the hassle.</p> <p>Here's what I recommend putting in your configuration file:</p> <pre><code>default-new-key-algo ed25519/cert\n</code></pre> <p>That will change the default to generate a single primary key capable only of certification. You are then free to make new subkeys as you deem useful, generally with only one function each.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#passwords","title":"Passwords","text":"<p>As always, the trick is to keep as many of your passwords as possible in a password manager or other form of encrypted state, then focus on memorizing only what you need to unlock the rest. If password A can be used to access password B, don't bother memorizing password B.</p> <p>Most of your passwords should be completely random. The ones you need to memorize should instead use diceware. Keep the number you need to remember as low as possible (single digits), and drill yourself on them regularly: I test my memory of those passwords monthly at minimum, halving the period each time I get it wrong and doubling it each time I get it right.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#expiration","title":"Expiration","text":"<p>Every single key should have an expiration date, especially primary keys. If this sounds excessive, it is because you misunderstand the meaning of expiration.</p> <p>An expired key should not be considered no longer valid: that is what revocation is for. Instead, an expired key should be considered outdated, and in need of a refresh. Unless you forget (do set reminders) or die, your key should not become outdated: you can simply take the primary key and extend the expiration date.</p> <p>Expiration dates are for the copies everyone else has: when you make changes to your key, others do not magically find those changes. An expiration date is a way to inform others that they need to get a newer version, in case something changed. Without one, an old copy of your key could be floating around somewhere causing confusion indefinitely: with one, you have a cap on the amount of time it takes anyone using your key to see changes.</p> <p>I set expiration dates for keys based on when I expect an important change to happen (if I'm making a signing key for a computer I expect to stop using in a few months, I'm going to set it to expire around then).</p> <p>The vast majority of keys, of course, are intended for indefinite use without foreseeable changes. I generally set them to expire two years in the future, and set annual reminders to renew their expiration date each year. This allows a lot of cushion in case I am busy around then, and ensures anyone with my key has at least a year before they are forced to check for updates.</p> <p>If finding all the keys on all your devices sounds like a hassle, remember that you don't need the private keys to be renewed: all you need is the primary public key and private key. Whenever you add a copy of your public key somewhere you control (like a GitLab account, for instance), add a note to your renewal reminder that it'll need the fresh copy. In practice, this should take around ten minutes at worst even if you have a lot of places to update.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#revocation","title":"Revocation","text":"<p>Revocation is possibly the most misunderstood part of the entire standard: even GitHub, one of the most prominent users of GPG keys anywhere, only recently fixed their interpretation of it.</p> <p>Revocation is functionally similar to expiration, except it cannot be reversed. The meaning is quite different, however: while expiration indicates a key should be updated, revocation means a key should not be used again. Key revocation includes a reason, which is extremely important: unless it is revoked due to compromise, the key should still be considered trustworthy. A signature made with a key revoked due to no longer being used is a valid signature. Sure, it should be considered suspicious if the signature is newer than the revocation, but if the key was being used maliciously it'd be revoked as compromised.</p> <p>Any key you do not intend to renew should be revoked. If you're never going to use a key again, revoke it and say so. If you're transitioning to a new key, revoke it and put the fingerprint (or the authoritative source, if you prefer) of the replacement in the revocation comment. It does not matter if it has already expired: when someone with an old copy of your key gets the current version, they need to understand how to proceed. If the latest version is also expired, all they can tell is that you've forgotten about it.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#the-most-important-feature-no-one-uses","title":"The Most Important Feature No One Uses","text":"<p>Whether you use GnuPG as a tool or a toy is determined primarily by whether anyone else actually has a current version of your key. Without that, it is almost entirely useless except as a particularly convoluted approach to encryption.</p> <p>Over the years, many people have worked tirelessly to make sharing keys more complicated and difficult than it should be, and they've been broadly successful. Humor me for a moment, and forget about public keyservers. Forget about the distributed keypools, forget about TOFU, forget about the big complicated programs people have made for this.</p> <p>Now, look at your key's preferences. You see that option to set a \"preferred keyserver\"? That is one of the most important and neglected parts of the entire OpenPGP specification.</p> <p>Set it to the URL where someone can get your key. This will become the One True Key, the canonical, authoritative, ultimate arbiter of what your public key consists of. Every time anything about your public key is changed, you will update this. It will contain every single element you consider part of your key, without exceptions. Contrary to the name, it does not have to be a keyserver. In fact, I recommend against it! Instead, simply set it to the URL of your exported public key. I host my keys named by fingerprint at the <code>openpgpkey</code> subdomain of my personal domain, accessible over direct HTTPS, secured by Let's Encrypt. That subdomain is also used for WKD, which I'll get to in a bit.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#finding-your-key-given-a-key","title":"Finding your key, given a key","text":"<p>Because you set the preferred keyserver field, anyone who has an old copy of your key can trivially get the newest copy, complete with revocations, additions, transitions, etcetera. In fact, GnuPG can do it for them, if they set <code>--keyserver-options honor-keyserver-url</code>!</p> <p>No matter how anyone actually gets your key, once they have it, they'll be able to use this source in the future. All other sources are merely backups for this one. Even if you forget to update other copies, they'll still point here, and that's all they really have to do.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#finding-your-key-given-a-signature","title":"Finding your key, given a signature","text":"<p>We're not done yet: when you sign something (like a Git commit), you are doing so to make sure others can verify it. If they can't, it's just theater. So you need to make sure that's all they need.</p> <p>Thankfully, GnuPG already has solutions for this. In order of precedence:</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#1-include-key-block","title":"1. <code>--include-key-block</code>","text":"<p>This configuration causes every signature you make to include a subset of your key. This is exceptionally inefficient given how big these keys tend to get, and I recommend against it. It strips key signatures, so the verifier can't really tell how trustworthy you are using the Web of Trust that underpins PGP. The only real merit it has is allowing the verifier to use other approaches, such as the preferred keyserver URL, to fetch the rest of the key. Speaking of which\u2026</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#2-sig-keyserver-url","title":"2. <code>--sig-keyserver-url</code>","text":"<p>Welcome to one of the runners-up for Most Important Feature No One Uses. For the low price of a few bytes, every signature you make can contain the definitive source of your entire public key, complete with all the key signatures from others, allowing the verifier to actually *gasp* verify the signature.</p> <p>Because I have this in my configuration file, my signatures carry actual value. When someone pulls a Git repository and runs <code>git log --show-signature</code>, assuming they have GnuPG set to auto-retrieve keys, Git will automatically fetch my current key and validate my commits according to the trust system of the local keyring.</p> <p>In fact, as a demonstration of how powerful this is, I've signed this post using my default settings and put the signature in a comment. Assuming you have a copy of GnuPG 2.3.6 with a default configuration in your path, you can pass the plain text content of this post into the following POSIX shell command and watch it automatically retrieve and evaluate my key to verify the signature.</p> <pre><code>gpg --auto-key-retrieve --keyserver-options honor-keyserver-url --verify\n</code></pre> <p>Yes, that can be used as a \"web bug\". I could also embed an image in this post and accomplish the exact same thing using your web browser, so I don't think that's a particularly serious problem.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#3-sender","title":"3. <code>--sender</code>","text":"<p>This embeds one of your UIDs into the signature. It's a poor substitute for a URL in my mind, as it passes the buck to the section below for key retrieval. Nevertheless, you might find it useful to specify a specific UID for some reason anyway. Keep in mind, however, that each signing key is tied to every UID of the primary key, so this doesn't actually mean much.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#4-nothing","title":"4. Nothing","text":"<p>With default signing options, you're left with just the fingerprint used to sign: see two sections down for how the verifier can proceed from there.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#finding-your-key-given-a-user-id-or-email","title":"Finding your key, given a user ID or email","text":"<p>If you have your own domain, and your primary UID is hosted on that domain, this is quite straightforward: use Web Key Directory and host a minimal subset of your key accordingly. If anyone has your email, they can then get that subset. From that subset, they can pull the full key from the preferred keyserver URL at their discretion.</p> <p>You could also configure a PGP CERT record to point directly to your canonical key file, thus saving them the trouble.</p> <p>If your primary UID can't be used with WKD, you're somewhat out of luck: pray they check a keyserver that has some form of your key floating around. If they do, they'll be able to fetch the latest version in the same manner.</p>","tags":["cryptography","gnupg","security"]},{"location":"gnupg-key-management/#finding-your-key-given-a-fingerprint","title":"Finding your key, given a fingerprint","text":"<p>Ideally, this never comes up: the main reason it would is if someone gets a public key you've signed and wants to know who signed it (a scenario rejected by GnuPG developers as unreasonable). As with UIDs on domains outside your control, you just have to hope they check a keyserver you've uploaded a copy to.</p> <p>I hope this helps others get more use out of GnuPG, which has gathered an undeserved reputation for being obsolete or unusable. I read a lot of guides on how to use GPG over time, but I can't recall almost any of them covering the points I have here. A lot of people report problems that can be solved using these principles, and I hope these options become less obscure in the future.</p>","tags":["cryptography","gnupg","security"]},{"location":"jupyterlab/","title":"JupyterLab","text":"","tags":["launchd","python","taskfile"]},{"location":"jupyterlab/#project-url","title":"Project URL","text":"<p>JupyterLab</p>","tags":["launchd","python","taskfile"]},{"location":"jupyterlab/#project-description","title":"Project description","text":"<p>JupyterLab is a highly extensible, feature-rich notebook authoring application and editing environment, and is a part of Project Jupyter, a large umbrella project centered around the goal of providing tools (and standards) for interactive computing with computational notebooks.</p> <p>A computational notebook is a shareable document that combines computer code, plain language descriptions, data, rich visualizations like 3D models, charts, graphs and figures, and interactive controls. A notebook, along with an editor like JupyterLab, provides a fast interactive environment for prototyping and explaining code, exploring and visualizing data, and sharing ideas with others.</p> <p>JupyterLab is a sibling to other notebook authoring applications under the Project Jupyter umbrella, like Jupyter Notebook and Jupyter Desktop. JupyterLab offers a more advanced, feature rich, customizable experience compared to Jupyter Notebook.</p>","tags":["launchd","python","taskfile"]},{"location":"jupyterlab/#installation-instructions","title":"Installation instructions","text":"<p>First, install Pixi. Please note that Pixi installation and configuration instructions are outside of the scope of this document, although if you use Homebrew, you may use it to quickly install Pixi:</p> <pre><code>brew install -- pixi\n</code></pre> <p>Ensure that the following directories exist:</p> <pre><code>mkdir -p -- \\\n  \"${HOME}/.jupyterlab\" \\\n  \"${HOME}/.taskfile/jupyterlab\" \\\n  \"${HOME}/Documents/Jupyter\" \\\n  \"${HOME}/Library/LaunchAgents\"\n</code></pre> <p>Place the Pixi manifest file for an isolated JupyterLab environment in the <code>~/.taskfile/jupyterlab</code> directory:</p> pixi.toml<pre><code># vim: set ft=toml :\n\n\n[project]\nname = 'jupyterlab'\nchannels = ['conda-forge']\nplatforms = [\n  # 'osx-64',\n  'osx-arm64',\n]\n\n\n[dependencies]\nipywidgets = '*'\njupyterlab = '*'\npixi-kernel = '*'\npython = '3.13.*'\n\n\n[pypi-dependencies]\njupyterlab-quarto = '*'\n</code></pre> <p>Create the JupyterLab environment, then remove JupyterLab's default kernels:</p> <pre><code>pushd -q -- \"${HOME}/.taskfile/jupyterlab\"\npixi update\npixi install\nfind -- .pixi/envs/default/share/jupyter/kernels -mindepth 1 -delete\npopd -q\n</code></pre> <p>Add this JupyterLab configuration file to the <code>~/.jupyter</code> directory:</p> jupyter_lab_config.py<pre><code># vim: set ft=python :\n\n\nfrom pathlib import Path\n\nc = get_config()  # noqa:F821\n\nc.ExtensionApp.open_browser = False\nc.FileContentsManager.preferred_dir = str(Path(\"Documents\").joinpath(\"Jupyter\"))\nc.IdentityProvider.token = \"\"\nc.KernelSpecManager.ensure_native_kernel = False\nc.ServerApp.ip = \"localhost\"\nc.ServerApp.local_hostnames = [\"localhost\"]\nc.ServerApp.open_browser = False\nc.ServerApp.port = 8888\nc.ServerApp.port_retries = 0\nc.ServerApp.root_dir = str(Path.home())\n</code></pre> <p>Now let's create a launchd service that will make it easy to automatically start and stop JupyterLab. Add this JupyterLab launchd service definition to the <code>~/Library/LaunchAgents</code> directory, editing usernames, pathnames and the <code>PATH</code> and <code>TZ</code> environment variables as needed:</p> org.jupyter.jupyterlab.server.plist<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!-- vim: set ft=xml : --&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n    &lt;key&gt;Disabled&lt;/key&gt;\n    &lt;false/&gt;\n    &lt;key&gt;EnvironmentVariables&lt;/key&gt;\n    &lt;dict&gt;\n        &lt;key&gt;PATH&lt;/key&gt;\n        &lt;string&gt;/Users/manselmi/.prefix/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin&lt;/string&gt;\n        &lt;key&gt;TZ&lt;/key&gt;\n        &lt;string&gt;UTC&lt;/string&gt;\n    &lt;/dict&gt;\n    &lt;key&gt;KeepAlive&lt;/key&gt;\n    &lt;dict&gt;\n        &lt;key&gt;SuccessfulExit&lt;/key&gt;\n        &lt;false/&gt;\n    &lt;/dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;org.jupyter.jupyterlab.server&lt;/string&gt;\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;/Users/manselmi/.taskfile/jupyterlab/.pixi/envs/default/bin/jupyter-lab&lt;/string&gt;\n    &lt;/array&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre> <p>If you would like to learn more about launchd, please see Creating Launch Daemons and Agents.</p> <p>The launchd service we created will run when loaded, so let's load the service:</p> <pre><code>launchctl bootstrap \"gui/$(id -u)/\" ~/Library/LaunchAgents/org.jupyter.jupyterlab.server.plist\n</code></pre> <p>Please note that upon future logins, the service will automatically be loaded and hence automatically started.</p> <p>Now let's confirm that JupyterLab is up and running. Navigate your browser to http://localhost:8888. If you see a JupyterLab page, then so far so good.</p>","tags":["launchd","python","taskfile"]},{"location":"jupyterlab/#register-python-environment-with-jupyterlab","title":"Register Python environment with JupyterLab","text":"<p>Let's create a Python environment specific to the <code>foo</code> project. In addition to Python, the environment must contain <code>ipykernel</code>.</p> <p>Create this project-specific Pixi manifest file:</p> pixi.toml<pre><code># vim: set ft=toml :\n\n\n[project]\nname = 'foo'\nchannels = ['conda-forge']\nplatforms = [\n  # 'osx-64',\n  'osx-arm64',\n]\n\n\n[dependencies]\nipykernel = '*'\npython = '3.13.*'\n</code></pre> <p>Create the Python environment:</p> <pre><code>pixi upgrade\npixi install\n</code></pre> <p>Create a kernel to register the environment with JupyterLab:</p> <pre><code>pixi run -- python -m ipykernel install --user \\\n  --name foo \\  # (1)!\n  --display-name Foo  # (2)!\n</code></pre> <ol> <li> <p>The kernel name is set to <code>foo</code>.</p> </li> <li> <p>The kernel display name is set to <code>Foo</code>.</p> </li> </ol> <p>Now let's confirm that we can start a notebook using our new kernel. Navigate your browser to http://localhost:8888. You should see a screen like this with the <code>Foo</code> kernel available.</p> <p></p> <p>Create a new notebook with the <code>Foo</code> kernel and run something simple like <code>1+1</code>.</p> <p></p> <p>Congratulations! \ud83e\udd73</p>","tags":["launchd","python","taskfile"]},{"location":"jupyterlab/#maintenance","title":"Maintenance","text":"<p>Routine maintenance tasks may be automated with Task. Please install it. Please note that Task installation and configuration instructions are outside of the scope of this document, although if you use Homebrew, you may use it to quickly install Task:</p> <pre><code>brew install -- go-task/tap/go-task\n</code></pre> <p>Place these taskfile directories in the <code>~/.taskfile/include</code> directory (create it if necessary), and ensure that the environment variable <code>TASKFILE_INCLUDE_ROOT_DIR</code> is set to the same directory, as in the example <code>~/.zshrc</code> configuration file.</p> <p>Additionally, place this JupyterLab taskfile in the <code>~/.taskfile/jupyterlab</code> directory:</p> taskfile.yaml<pre><code># vim: set ft=yaml :\n#\n# https://taskfile.dev\n\n\nversion: '3'\n\nset: ['errexit', 'nounset', 'pipefail']\n\nvars:\n  USER_ID:\n    sh: 'id -u'\n\nincludes:\n\n  launchctl:\n    taskfile: '{{ env \"TASKFILE_INCLUDE_ROOT_DIR\" }}/launchctl'\n    vars:\n      DOMAIN_TARGET: 'gui/{{ .USER_ID }}/'\n      SERVICE_NAME: 'org.jupyter.jupyterlab.server'\n      SERVICE_PATH: '{{ env \"HOME\" }}/Library/LaunchAgents/{{ .SERVICE_NAME }}.plist'\n\n  pixi:\n    taskfile: '{{ env \"TASKFILE_INCLUDE_ROOT_DIR\" }}/pixi'\n\ntasks:\n\n  upgrade:\n    aliases: ['default']\n    cmds:\n      - task: 'launchctl:bootout'\n      - task: 'pixi:update'\n      - task: 'pixi:install'\n      - task: 'remove-default-kernels'\n      - task: 'launchctl:bootstrap'\n\n  remove-default-kernels:\n    cmds:\n      - 'find -- .pixi/envs/default/share/jupyter/kernels -mindepth 1 -delete'\n</code></pre> <p>Here are some common tasks:</p> <p>To stop the JupyterLab service, run:</p> <pre><code>task -d \"${HOME}/.taskfile/jupyterlab\" launchctl:bootout\n</code></pre> <p>To start the JupyterLab service (if not already running), run:</p> <pre><code>task -d \"${HOME}/.taskfile/jupyterlab\" launchctl:bootstrap\n</code></pre> <p>To upgrade the JupyterLab environment and remove any default kernel, run:</p> <pre><code>pushd -q -- \"${HOME}/.taskfile/jupyterlab\"\ntask launchctl:bootout\ntask pixi:update\ntask pixi:install\ntask remove-default-kernels\ntask launchctl:bootstrap\npopd -q\n</code></pre> <p>To perform the previous operations with a single command, run:</p> <pre><code>task -d \"${HOME}/.taskfile/jupyterlab\" upgrade\n</code></pre> <p>To disable JupyterLab, within <code>~/Library/LaunchAgents/org.jupyter.jupyterlab.server.plist</code> change</p> <pre><code>    &lt;key&gt;Disabled&lt;/key&gt;\n    &lt;false/&gt;\n</code></pre> <p>to</p> <pre><code>    &lt;key&gt;Disabled&lt;/key&gt;\n    &lt;true/&gt;\n</code></pre> <p>then run:</p> <pre><code>task -d \"${HOME}/.taskfile/jupyterlab\" launchctl:bootout\n</code></pre> <p>To uninstall JupyterLab, first disable it and then run:</p> <pre><code>task -d \"${HOME}/.taskfile/jupyterlab\" pixi:clean\n</code></pre>","tags":["launchd","python","taskfile"]},{"location":"rclone/","title":"Rclone","text":"","tags":["rclone","security","ssh"]},{"location":"rclone/#introduction","title":"Introduction","text":"<p>The following introduction is sourced directly from rclone.org.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#about-rclone","title":"About Rclone","text":"<p>Rclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors' web storage interfaces. Over 70 cloud storage products support rclone including S3 object stores, business &amp; consumer file storage services, as well as standard transfer protocols.</p> <p>Rclone has powerful cloud equivalents to the unix commands rsync, cp, mv, mount, ls, ncdu, tree, rm, and cat. Rclone's familiar syntax includes shell pipeline support, and <code>--dry-run</code> protection. It is used at the command line, in scripts or via its API.</p> <p>Users call rclone \"The Swiss army knife of cloud storage\", and \"Technology indistinguishable from magic\".</p> <p>Rclone really looks after your data. It preserves timestamps and verifies checksums at all times. Transfers over limited bandwidth; intermittent connections, or subject to quota can be restarted, from the last good file transferred. You can check the integrity of your files. Where possible, rclone employs server-side transfers to minimise local bandwidth use and transfers from one provider to another without using local disk.</p> <p>Virtual backends wrap local and cloud file systems to apply encryption, compression, chunking, hashing and joining.</p> <p>Rclone mounts any local, cloud or virtual filesystem as a disk on Windows, macOS, linux and FreeBSD, and also serves these over SFTP, HTTP, WebDAV, FTP and DLNA.</p> <p>Rclone is mature, open-source software originally inspired by rsync and written in Go. The friendly support community is familiar with varied use cases. Official Ubuntu, Debian, Fedora, Brew and Chocolatey repos. include rclone. For the latest version downloading from rclone.org is recommended.</p> <p>Rclone is widely used on Linux, Windows and Mac. Third-party developers create innovative backup, restore, GUI and business process solutions using the rclone command line or API.</p> <p>Rclone does the heavy lifting of communicating with cloud storage.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#what-can-rclone-do-for-you","title":"What can rclone do for you?","text":"<p>Rclone helps you:</p> <ul> <li> <p>Backup (and encrypt) files to cloud storage</p> </li> <li> <p>Restore (and decrypt) files from cloud storage</p> </li> <li> <p>Mirror cloud data to other cloud services or locally</p> </li> <li> <p>Migrate data to the cloud, or between cloud storage vendors</p> </li> <li> <p>Mount multiple, encrypted, cached or diverse cloud storage as a disk</p> </li> <li> <p>Analyse and account for data held on cloud storage   using lsf,   ljson,   size, ncdu</p> </li> <li> <p>Union file systems together to present multiple local and/or cloud   file systems as one</p> </li> </ul>","tags":["rclone","security","ssh"]},{"location":"rclone/#features","title":"Features","text":"<ul> <li> <p>Transfers</p> <ul> <li> <p>MD5, SHA1 hashes are checked at all times for file integrity</p> </li> <li> <p>Timestamps are preserved on files</p> </li> <li> <p>Operations can be restarted at any time</p> </li> <li> <p>Can be to and from network, e.g. two different cloud providers</p> </li> <li> <p>Can use multi-threaded downloads to local disk</p> </li> </ul> </li> <li> <p>Copy new or changed files to cloud storage</p> </li> <li> <p>Sync (one way) to make a directory identical</p> </li> <li> <p>Bisync (two way) to keep two directories in sync   bidirectionally</p> </li> <li> <p>Move files to cloud storage deleting the local after   verification</p> </li> <li> <p>Check hashes and for missing/extra files</p> </li> <li> <p>Mount your cloud storage as a network disk</p> </li> <li> <p>Serve local or remote   files over HTTP   / WebDav   / FTP /   SFTP /   DLNA</p> </li> <li> <p>Experimental Web based GUI</p> </li> </ul>","tags":["rclone","security","ssh"]},{"location":"rclone/#installation","title":"Installation","text":"","tags":["rclone","security","ssh"]},{"location":"rclone/#rclone_1","title":"Rclone","text":"<p>Rclone's macOS installation options are here, but please avoid installing via Homebrew and instead follow the instructions to install a pre-compiled binary. At this time, skip any step involving running <code>rclone config</code>, as that's covered later on this page, and also replace the Rclone download URL with the one corresponding to your CPU architecture here.</p> <p>On macOS, depending on the installation method, the <code>rclone</code> binary may have the <code>com.apple.quarantine</code> extended attribute, which needs to be deleted. Delete the extended attribute by running</p> <pre><code>xattr -d com.apple.quarantine rclone  # (1)!\n</code></pre> <ol> <li>If the <code>rclone</code> binary is not in the current directory, replace <code>rclone</code> with its actual    location.</li> </ol> <p>Also, on macOS, grant the <code>rclone</code> binary and your terminal app(s) Full Disk Access. Confirm Full Disk Access by running the following:</p> <pre><code>sqlite3 \\\n    '/Library/Application Support/com.apple.TCC/TCC.db' \\\n    'SELECT client FROM access WHERE auth_value AND service = \"kTCCServiceSystemPolicyAllFiles\"' \\\n  | grep -Ei 'rclone|term' \\\n  | sort -f\n</code></pre> <p>The output should look similar to this:</p> <pre><code>/Users/manselmi/.prefix/bin/rclone\ncom.apple.Terminal\ncom.googlecode.iterm2\n</code></pre>","tags":["rclone","security","ssh"]},{"location":"rclone/#fuse-t","title":"FUSE-T","text":"<p>On macOS, Rclone leverages FUSE-T to locally mount a remote location.</p> <p>Please review FUSE-T's installation options here.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#example-usage-and-configuration","title":"Example usage and configuration","text":"<p>Suppose you would like to locally mount your VPS home directory (e.g. <code>/home/ubuntu/</code>). In this example you will configure Rclone such that after running a command like</p> <pre><code>rclone mount vps-home: ~/mnt/vps-home/ \u2026\n</code></pre> <p>any changes within the local directory <code>~/mnt/vps-home/</code> will be propagated to the remote directory <code>/home/ubuntu/</code> and vice versa.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#create-ssh-key-pair","title":"Create SSH key pair","text":"<p>First, create an SSH key pair so that Rclone may SSH to the VPS without requiring user input after Rclone has been started.</p> <pre><code>ssh-keygen \\\n  -t ed25519 \\\n  -C 'Rclone authentication key (Ed25519)' \\\n  -f ~/.ssh/keys/rclone-auth-ed25519\n</code></pre> <p>Do not set a passphrase for the private key. The private key will reside within Rclone's configuration file <code>~/.config/rclone/rclone.conf</code>, the entirety of which will be encrypted.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#append-public-key-to-remote-sshauthorized_keys","title":"Append public key to remote <code>~/.ssh/authorized_keys</code>","text":"<p>This key pair will be used only by Rclone to run commands remotely, so we can enable all restrictions (e.g. disable port, agent and X11 forwarding; disable PTY allocation; disable execution of <code>~/.ssh/rc</code>) as described in the SSH authorized keys file format.</p> <p>Run the following to append the restricted public key to the file <code>~/.ssh/authorized_keys</code> within your VPS home directory.</p> <pre><code>read -r -d '' SHELL_PROG &lt;&lt; 'EOF' || true\n# Stop at any error.\nset -o errexit\n\n# Create ~/.ssh directory if it doesn't exist.\nmkdir -p ~/.ssh\n\n# Append stdin to ~/.ssh/authorized_keys.\ncat &gt;&gt; ~/.ssh/authorized_keys\n\n# Restrict ~/.ssh directory permissions.\nchmod -R go= ~/.ssh\nEOF\n\nprintf '%s %s\\n' restrict \"$(cat ~/.ssh/keys/rclone-auth-ed25519.pub)\" \\\n  | ssh -o RequestTTY=no vps \"${SHELL_PROG}\"  # (1)!\n</code></pre> <ol> <li>Replace <code>vps</code> with the actual SSH destination.</li> </ol>","tags":["rclone","security","ssh"]},{"location":"rclone/#create-rclone-configuration-file","title":"Create Rclone configuration file","text":"<p>First, create an empty Rclone configuration file.</p> <pre><code>mkdir -p ~/.config/rclone\ntouch ~/.config/rclone/rclone.conf\n</code></pre> <p>Add the following to <code>~/.config/rclone/rclone.conf</code>:</p> <pre><code>[vps]\ntype = sftp\nhost = vps.manselmi.com\nuser = manselmi\nkey_pem = XXX  # (1)!\nknown_hosts_file = ~/.ssh/known_hosts\nshell_type = unix\nmd5sum_command = md5sum\nsha1sum_command = sha1sum\nchunk_size = 255Ki\nconcurrency = 1\n\n[vps-home]\ntype = alias\nremote = vps:/home/ubuntu/\n</code></pre> <ol> <li>Replace <code>XXX</code> with the output of <code>&lt; ~/.ssh/keys/rclone-auth-ed25519 awk '{printf \"%s\\\\n\", $0}'</code></li> </ol> <p>Please see the following sections of the Rclone documentation to learn more:</p> <ul> <li> <p>rclone config</p> </li> <li> <p>SFTP</p> </li> <li> <p>Alias</p> </li> </ul>","tags":["rclone","security","ssh"]},{"location":"rclone/#encrypt-rclone-configuration-file","title":"Encrypt Rclone configuration file","text":"<p>First, generate a random password, which will be printed to standard output.</p> <pre><code>read -r -d '' PYTHON_PROG &lt;&lt; 'EOF' || true\nimport secrets\nimport string\nalphabet = string.ascii_letters + string.digits + string.punctuation\nprint(\"\".join(secrets.choice(alphabet) for _ in range(64)))\nEOF\n\npython -c \"${PYTHON_PROG}\"\n</code></pre> <p>Copy the password, then store it in your macOS user's default keychain by running the following command and pasting the password when prompted.</p> <pre><code>/usr/bin/security add-generic-password \\\n  -s rclone \\\n  -a rclone.conf \\\n  -w\n</code></pre> <p>Before moving on, confirm you can retrieve the password from the default keychain.</p> <pre><code>/usr/bin/security find-generic-password \\\n  -s rclone \\\n  -a rclone.conf \\\n  -w\n</code></pre>","tags":["rclone","security","ssh"]},{"location":"rclone/#update-shell-configuration-file","title":"Update shell configuration file","text":"<p>Include the following Rclone password command and helper functions in your shell configuration file, which on macOS is likely <code>~/.zshrc</code>. Afterwards, re-execute the shell configuration file by running <code>source ~/.zshrc</code>.</p> <pre><code># https://rclone.org/docs/#configuration-encryption\nexport RCLONE_PASSWORD_COMMAND='/usr/bin/security find-generic-password -s rclone -a rclone.conf -w'\n\n_rclone-mount() {\n  local REMOTE\n\n  if [[ -z \"${1-}\" ]]; then\n    printf '%s\\n' 'ERROR: _rclone-mount expects the name of an rclone remote' &gt;&amp;2\n    return 1\n  fi\n  REMOTE=\"${1}\"\n\n  # https://rclone.org/commands/rclone_mount/#fuse-t-limitations-caveats-and-notes\n  rclone mount \"${REMOTE}:\" \"${HOME}/mnt/${REMOTE}/\" \\\n    --vfs-cache-mode=full \\\n    --vfs-write-back=5s \\  # (1)!\n    --volname=\"${REMOTE}\"\n}\n\n# Mount rclone remote \"vps-home\" to ~/mnt/vps-home/\nmnt-vps-home() {\n  _rclone-mount vps-home\n}\n</code></pre> <ol> <li>Time to writeback files after last use when using cache (default <code>5s</code>) - adjust as desired</li> </ol>","tags":["rclone","security","ssh"]},{"location":"rclone/#encrypt-rclone-configuration-file_1","title":"Encrypt Rclone configuration file","text":"<p>Run <code>rclone config</code> to launch the interactive configuration editor, then press <code>s</code> to set configuration password, then press <code>a</code> to add password. Provide the password from earlier when prompted. Press <code>q</code> to quit to main menu, then press <code>q</code> again to quit the configuration editor.</p> <p>Confirm that the file has been encrypted by running <code>less ~/.config/rclone/rclone.conf</code>. The output should begin with this:</p> <pre><code># Encrypted rclone configuration File\n\nRCLONE_ENCRYPT_V0:\n</code></pre> <p>Also, confirm that the file may be decrypted on-demand by running <code>rclone config redacted</code>. Sensitive values will automatically be redacted.</p>","tags":["rclone","security","ssh"]},{"location":"rclone/#delete-ssh-private-key","title":"Delete SSH private key","text":"<p>It's now safe to turn off your computer delete your private key.</p> <pre><code>rm ~/.ssh/keys/rclone-auth-ed25519\n</code></pre>","tags":["rclone","security","ssh"]},{"location":"rclone/#create-mount-point","title":"Create mount point","text":"<p>Create the local directory to which the remote directory will be mounted.</p> <pre><code>mkdir -p ~/mnt/vps-home\nchmod -R go= ~/mnt\n</code></pre>","tags":["rclone","security","ssh"]},{"location":"rclone/#mount-remote-directory","title":"Mount remote directory","text":"<p>Run the following:</p> <pre><code>mnt-vps-home\n</code></pre> <p>In a separate terminal, confirm a successful mount by running this:</p> <pre><code>mount | grep -F vps-home\n</code></pre> <p>The output should look similar to this:</p> <pre><code>fuse-t:/vps-home on /Users/manselmi/mnt/vps-home (nfs, nodev, nosuid, mounted by manselmi)\n</code></pre>","tags":["rclone","security","ssh"]},{"location":"rclone/#additional-usage","title":"Additional usage","text":"","tags":["rclone","security","ssh"]},{"location":"rclone/#tag:rclone","title":"rclone","text":"<ul> <li>            Automated backup          </li> </ul>","tags":["rclone","security","ssh"]},{"location":"tags/","title":"Tags","text":"<p>The following is a list of pages grouped by tag:</p>"},{"location":"tags/#tag:aws","title":"aws","text":"<ul> <li>            aws-vault          </li> </ul>"},{"location":"tags/#tag:cryptography","title":"cryptography","text":"<ul> <li>            GnuPG key management          </li> </ul>"},{"location":"tags/#tag:git","title":"git","text":"<ul> <li>            Git          </li> </ul>"},{"location":"tags/#tag:gnupg","title":"gnupg","text":"<ul> <li>            GnuPG key management          </li> </ul>"},{"location":"tags/#tag:launchd","title":"launchd","text":"<ul> <li>            Automated backup          </li> <li>            JupyterLab          </li> <li>            devpi          </li> </ul>"},{"location":"tags/#tag:mermaid","title":"mermaid","text":"<ul> <li>            Automated backup          </li> <li>            devpi          </li> </ul>"},{"location":"tags/#tag:python","title":"python","text":"<ul> <li>            JupyterLab          </li> <li>            devpi          </li> </ul>"},{"location":"tags/#tag:rclone","title":"rclone","text":"<ul> <li>            Automated backup          </li> <li>            Rclone          </li> </ul>"},{"location":"tags/#tag:security","title":"security","text":"<ul> <li>            Automated backup          </li> <li>            GnuPG key management          </li> <li>            Rclone          </li> <li>            aws-vault          </li> </ul>"},{"location":"tags/#tag:shell","title":"shell","text":"<ul> <li>            Git          </li> <li>            tar over SSH          </li> <li>            tmux environment variable sync          </li> </ul>"},{"location":"tags/#tag:ssh","title":"ssh","text":"<ul> <li>            Rclone          </li> <li>            Windows Subsystem for Linux          </li> <li>            tar over SSH          </li> <li>            tmux environment variable sync          </li> </ul>"},{"location":"tags/#tag:taskfile","title":"taskfile","text":"<ul> <li>            JupyterLab          </li> </ul>"},{"location":"tags/#tag:tmux","title":"tmux","text":"<ul> <li>            tmux environment variable sync          </li> </ul>"},{"location":"tags/#tag:windows","title":"windows","text":"<ul> <li>            Windows Subsystem for Linux          </li> </ul>"},{"location":"tar-ssh/","title":"tar over SSH","text":"","tags":["shell","ssh"]},{"location":"tar-ssh/#introduction","title":"Introduction","text":"<p>As described on my backup page, I headlessly manage backups of my family's Macs over SSH, and occasionally I need to add or update configuration files, scripts and binaries. Instead of dealing with files individually, I prefer to transfer all relevant files in one operation, make changes locally, then transfer everything back in one operation, preserving file ownership and permissions. This is possible by running tar over SSH.</p> <p>What follows is an example of managing backup-related files on a family member's Mac laptop.</p>","tags":["shell","ssh"]},{"location":"tar-ssh/#example","title":"Example","text":"<p>Note</p> <p>All commands are run on my own Mac, not on the remote Mac.</p> <p>Set some shell variables that we'll reference throughout.</p> <pre><code>SSH_ALIAS=foo-mac  # (1)!\nMANIFEST_ORIGINAL=\"${SSH_ALIAS}.original.manifest\"  # (2)!\nMANIFEST_MODIFIED=\"${SSH_ALIAS}.modified.manifest\"  # (3)!\nTAR_ORIGINAL=\"${SSH_ALIAS}.original.tar\"  # (4)!\nTAR_MODIFIED=\"${SSH_ALIAS}.modified.tar\"  # (5)!\nUSER_MAP=\"${SSH_ALIAS}.user.map\"  # (6)!\nGROUP_MAP=\"${SSH_ALIAS}.group.map\"  # (7)!\n</code></pre> <ol> <li> <p>This is an SSH host alias in <code>~/.ssh/config</code> such that I can SSH to the remote Mac via <code>ssh    foo-mac</code>.</p> </li> <li> <p>This file is a manifest of remote files and directories to transfer locally.</p> </li> <li> <p>This file is a manifest of local files and directories to transfer remotely.</p> </li> <li> <p>This file is a TAR of the files in <code>${MANIFEST_ORIGINAL}</code>.</p> </li> <li> <p>This file is a TAR of the files in <code>${MANIFEST_MODIFIED}</code> to be transferred remotely.</p> </li> <li> <p>This file maps local user IDs to user names and user IDs within the TAR.</p> </li> <li> <p>This file maps local group IDs to group names and group IDs within the TAR.</p> </li> </ol> <p>Before going further, define these shell functions as aliases of the following programs.</p> <ul> <li><code>awk</code> \u2192 <code>gawk</code> (GNU awk)</li> <li><code>find</code> \u2192 <code>gfind</code> (GNU find)</li> <li><code>sed</code> \u2192 <code>gsed</code> (GNU sed)</li> <li><code>tar</code> \u2192 <code>gtar</code> (GNU tar)</li> </ul> <pre><code>awk() { \"$(whence -p gawk)\" \"${@}\" ; }\nfind() { \"$(whence -p gfind)\" \"${@}\" ; }\nsed() { \"$(whence -p gsed)\" \"${@}\" ; }\ntar() { \"$(whence -p gtar)\" --format=posix \"${@}\" ; }\n</code></pre> <p>Create a file manifest\u2026</p> <pre><code>touch -- \"${MANIFEST_ORIGINAL}\"\n</code></pre> <p>\u2026and add resolved absolute pathnames of the files to archive. Pathnames must not have a trailing slash.</p> foo-mac.original.manifest<pre><code># vim: set ft=cfg :\n\n\n/Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist\n/Users/foo/.config/rclone\n/Users/foo/.config/resticprofile\n/Users/foo/Library/LaunchAgents/com.manselmi.resticprofile.foo_mac.backup.plist\n/usr/local/bin/exec-rclone\n/usr/local/bin/exec-resticprofile\n/usr/local/bin/rclone\n/usr/local/bin/restic\n/usr/local/bin/resticprofile\n</code></pre> <p>Create this Zsh script, which accepts pathnames on stdin and emits a TAR on stdout. See comments for details.</p> tar-create.sh<pre><code>#!/usr/bin/env -S -- zsh -f\n# vim: set ft=zsh :\n\n# Stop at any error, treat unset vars as errors and make pipelines exit with a non-zero exit code if\n# any command in the pipeline exits with a non-zero exit code.\nset -o ERR_EXIT\nset -o NO_UNSET\nset -o PIPE_FAIL\n\n\n# If macOS, define the following shell functions as aliases of the following programs (available via\n# Homebrew):\n#\n#   awk \u2192 gawk (GNU awk)\n#   find \u2192 gfind (GNU find)\n#   sed \u2192 gsed (GNU sed)\n#   tar \u2192 gtar (GNU tar)\n#\n# https://zsh.sourceforge.io/Doc/Release/Shell-Builtin-Commands.html#index-whence\nif [[ \"${OSTYPE}\" == darwin* ]]; then\n  awk() { \"$(whence -p gawk)\" \"${@}\" ; }\n  find() { \"$(whence -p gfind)\" \"${@}\" ; }\n  sed() { \"$(whence -p gsed)\" \"${@}\" ; }\n  tar() { \"$(whence -p gtar)\" --format=posix \"${@}\" ; }\nfi\n\n# For each input pathname such as\n#\n#   foo/bar/baz\n#\n# print\n#\n#   foo\n#   foo/bar\n#   foo/bar/baz\n#\n# When combined with GNU tar's `--no-recursion` option, this allows us to ensure inclusion of parent\n# directories in order to, upon extraction and if necessary, create missing parent directories with\n# correct ownership and permissions.\n#\n# https://www.gnu.org/software/tar/manual/html_section/recurse.html\n# https://serverfault.com/a/877313\nread -r -d '' AWK_PROG &lt;&lt; 'EOF' || true\nBEGIN {\n  FS = \"/\"\n  RS = \"\\0\"\n  ORS = \"\\0\"\n}\n{\n  path_component = $1\n  for (i = 2; i &lt;= NF; i++) {\n    print path_component\n    path_component = path_component \"/\" $i\n  }\n  print path_component\n}\nEOF\n\n# 1. `sed`: Accept null-byte-terminated pathnames from stdin and strip a leading slash (`/`) from\n#    each pathname.\n#\n# 2. `find`: Relative to ${TAR_DIRECTORY}, search the directory trees rooted at the supplied\n#    pathnames and print the pathnames of directories, regular files and symlinks pointing to\n#    directories or regular files. Suppress warnings regarding nonexistent files.\n#\n# 3. `awk`: Run the AWK program described above if ${TAR_PARENT_DIRS} is non-empty, otherwise no-op.\n#\n# 4. `sort`: Sort and deduplicate pathnames.\n#\n# 5. `tar`: Archive files with given pathnames relative to ${TAR_DIRECTORY}, suppressing warnings\n#    regarding files that cannot be read. Print the archive to stdout.\ntypeset -a TAR_PARENT_DIRS_CMD\nTAR_PARENT_DIRS_CMD=('cat')\nif [[ -n \"${TAR_PARENT_DIRS-}\" ]]; then\n  TAR_PARENT_DIRS_CMD=('awk' '--' \"${AWK_PROG}\")\nfi\nsed -z -- 's|^/||' \\\n  | (\n      pushd -q -- \"${TAR_DIRECTORY}\"\n      find -- -files0-from - -xtype d,f -print0 2&gt; /dev/null\n    ) \\\n  | \"${TAR_PARENT_DIRS_CMD[@]}\" \\\n  | env -- LC_ALL=POSIX sort -uz \\\n  | tar \\\n      -cf - \\\n      ${TAR_USER_MAP:+\"--owner-map=${TAR_USER_MAP}\"} \\\n      ${TAR_GROUP_MAP:+\"--group-map=${TAR_GROUP_MAP}\"} \\\n      --directory=\"${TAR_DIRECTORY}\" \\\n      --format=posix \\\n      --ignore-failed-read \\\n      --warning=no-failed-read \\\n      --no-recursion \\\n      --sort=none \\\n      --null \\\n      --files-from=-\n</code></pre> <p>Run the script remotely over SSH, optionally piping the output through Pipe Viewer to monitor throughput. Here, <code>printf</code> is a Zsh builtin.</p> <pre><code>&lt; \"${MANIFEST_ORIGINAL}\" \\\n    sed -E -- '/^[[:blank:]]*(#|$)/d' \\\n  | tr '\\n' '\\0' \\\n  | ssh -o RequestTTY=no -- \"${SSH_ALIAS}\" sudo -- env -- \\\n      PATH='/var/manselmi/.prefix/bin:/var/manselmi/.prefix/sw/homebrew/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin' \\\n      TAR_DIRECTORY=/ \\\n      TAR_PARENT_DIRS=1 \\\n      zsh -fc \"$(printf '%q' \"$(&lt; tar-create.sh)\")\" \\\n  | pv -W \\\n  &gt; \"${TAR_ORIGINAL}\"\n</code></pre> <p>We now have a TAR we can inspect. List the archive members and ensure what's needed is present.</p> <pre><code>tar -tf \"${TAR_ORIGINAL}\" --verbose | less -S\n</code></pre> <pre><code>drwxr-xr-x root/wheel        0 2024-01-06 00:10 Library/\ndrwxr-xr-x root/wheel        0 2024-01-20 21:25 Library/Preferences/\n-rw-r--r-- root/wheel      420 2024-01-19 14:30 Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist\ndrwxr-xr-x root/admin        0 2024-01-05 21:35 Users/\ndrwxr-x--- foo/staff         0 2024-01-20 21:26 Users/foo/\ndrwxr-xr-x foo/staff         0 2024-01-20 21:25 Users/foo/.config/\ndrwxr-xr-x foo/staff         0 2024-01-05 20:56 Users/foo/.config/rclone/\n-rw------- foo/staff       602 2024-01-14 22:25 Users/foo/.config/rclone/rclone.conf\ndrwxr-xr-x foo/staff         0 2024-01-19 14:30 Users/foo/.config/resticprofile/\ndrwxr-xr-x foo/staff         0 2024-01-14 22:28 Users/foo/.config/resticprofile/curlrc/\n-rw-r--r-- foo/staff       192 2024-01-05 20:30 Users/foo/.config/resticprofile/curlrc/foo_mac\ndrwxr-xr-x foo/staff         0 2024-01-14 22:28 Users/foo/.config/resticprofile/exclude/\n-rw-r--r-- foo/staff       984 2023-12-18 08:22 Users/foo/.config/resticprofile/exclude/base.txt\n-rw-r--r-- foo/staff       813 2024-01-06 04:03 Users/foo/.config/resticprofile/exclude/foo_mac.txt\ndrwxr-xr-x foo/staff         0 2024-01-14 22:28 Users/foo/.config/resticprofile/log/\ndrwxr-xr-x foo/staff         0 2023-08-20 18:02 Users/foo/.config/resticprofile/log/foo_mac/\n-rw-r--r-- foo/staff      4695 2024-01-20 14:03 Users/foo/.config/resticprofile/log/foo_mac/backup.log\n-rw-r--r-- foo/staff      2994 2024-01-19 14:30 Users/foo/.config/resticprofile/profiles.toml\ndrwxr-xr-x foo/staff         0 2024-01-14 22:31 Users/foo/.config/resticprofile/status/\n-rw-r--r-- foo/staff      2461 2024-01-20 14:03 Users/foo/.config/resticprofile/status/foo_mac.json\ndrwx------ foo/staff         0 2024-01-07 16:41 Users/foo/Library/\ndrwx------ foo/staff         0 2024-01-14 22:30 Users/foo/Library/LaunchAgents/\n-rw-r--r-- foo/staff      1005 2024-01-14 22:30 Users/foo/Library/LaunchAgents/com.manselmi.resticprofile.foo_mac.backup.plist\ndrwxr-xr-x root/wheel        0 2023-12-15 09:43 usr/\ndrwxr-xr-x root/wheel        0 2024-01-06 03:57 usr/local/\ndrwxr-xr-x root/wheel        0 2024-01-14 22:05 usr/local/bin/\n-rwxr-xr-x root/wheel      397 2024-01-05 22:53 usr/local/bin/exec-rclone\n-rwxr-xr-x root/wheel      533 2024-01-05 22:53 usr/local/bin/exec-resticprofile\n-rwxr-xr-x root/wheel 73065456 2024-01-08 06:19 usr/local/bin/rclone\n-rwxr-xr-x root/wheel 27146176 2024-01-14 17:43 usr/local/bin/restic\n-rwxr-xr-x root/wheel 16102320 2023-10-24 11:54 usr/local/bin/resticprofile\n</code></pre> <p>Before extracting the archive, print a deduplicated table of member user IDs, group IDs, user names and group names. We'll need this later.</p> <pre><code>paste \\\n    &lt;(\n      tar -tf \"${TAR_ORIGINAL}\" --quoting-style=escape --verbose --numeric-owner \\\n        | tr -s '[[:blank:]]' '\\t' \\\n        | cut -f 2\n    ) \\\n    &lt;(\n      tar -tf \"${TAR_ORIGINAL}\" --quoting-style=escape --verbose \\\n        | tr -s '[[:blank:]]' '\\t' \\\n        | cut -f 2\n    ) \\\n  | awk -F '[[:blank:]/]' -- '{ print $1, $2, $3, $4 }' \\\n  | {\n      printf '%s\\t%s\\t%s\\t%s\\n' UID GID UNAME GNAME\n      sort -u -k 1,1n -k 2,2n\n    } \\\n  | column -t\n</code></pre> <pre><code>UID  GID  UNAME  GNAME\n0    0    root   wheel\n0    80   root   admin\n501  20   foo    staff\n</code></pre> <p>Create this Zsh script. See comments for details.</p> tar-extract.sh<pre><code>#!/usr/bin/env -S -- zsh -f\n# vim: set ft=zsh :\n\n# Stop at any error, treat unset vars as errors and make pipelines exit with a non-zero exit code if\n# any command in the pipeline exits with a non-zero exit code.\nset -o ERR_EXIT\nset -o NO_UNSET\nset -o PIPE_FAIL\n\n\n# If macOS, define the following shell functions as aliases of the following programs (available via\n# Homebrew):\n#\n#   tar \u2192 gtar (GNU tar)\n#\n# https://zsh.sourceforge.io/Doc/Release/Shell-Builtin-Commands.html#index-whence\nif [[ \"${OSTYPE}\" == darwin* ]]; then\n  tar() { \"$(whence -p gtar)\" --format=posix \"${@}\" ; }\nfi\n\n# Accept TAR from stdin and extract relative to ${TAR_DIRECTORY}, preserving ownership and\n# permissions.\ntar \\\n  -xf - \\\n  --directory=\"${TAR_DIRECTORY}\" \\\n  --same-owner \\\n  --same-permissions\n</code></pre> <p>Create the directory <code>${SSH_ALIAS}</code> and extract the archive into it, preserving member ownership and permission.</p> <pre><code>sudo -- rm -fr -- \"${SSH_ALIAS}\"\nmkdir -- \"${SSH_ALIAS}\"\n&lt; \"${TAR_ORIGINAL}\" sudo -- env -- TAR_DIRECTORY=\"${SSH_ALIAS}\" ./tar-extract.sh\n</code></pre> <p>We're now ready to create or modify files as needed. <code>sudo</code> may be required to view or modify files not owned by our user.</p> <pre><code>sudo -u \\#501 -g \\#20 -- mkdir -- \"${SSH_ALIAS}/Users/foo/.config/foo\"\nsudo -u \\#501 -g \\#20 -- touch -- \"${SSH_ALIAS}/Users/foo/.config/foo/bar\"\nsudo -u \\#501 -g \\#20 -- touch -- \"${SSH_ALIAS}/Users/foo/.config/foobar\"\npushd -q -- \"${SSH_ALIAS}/Users/foo/.config\"\nsudo -u \\#501 -g \\#20 -- ln -s foo baz\npopd -q\n</code></pre> <p>Now that we've modified the necessary files, let's prepare to archive them. First, make a copy of the original manifest and add any new files to include them in the archive. Pathnames must not have a trailing slash.</p> <pre><code>cp -- \"${MANIFEST_ORIGINAL}\" \"${MANIFEST_MODIFIED}\"\n\ncat &gt;&gt; \"${MANIFEST_MODIFIED}\" &lt;&lt; 'EOF'\n/Users/foo/.config/baz\n/Users/foo/.config/foo\n/Users/foo/.config/foobar\nEOF\n</code></pre> foo-mac.modified.manifest<pre><code># vim: set ft=cfg :\n\n\n/Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist\n/Users/foo/.config/rclone\n/Users/foo/.config/resticprofile\n/Users/foo/Library/LaunchAgents/com.manselmi.resticprofile.foo_mac.backup.plist\n/usr/local/bin/exec-rclone\n/usr/local/bin/exec-resticprofile\n/usr/local/bin/rclone\n/usr/local/bin/restic\n/usr/local/bin/resticprofile\n/Users/foo/.config/baz\n/Users/foo/.config/foo\n/Users/foo/.config/foobar\n</code></pre> <p>Create files that map local user names/IDs to remote user names and user IDs, and local group names/IDs to remote group names and group IDs, respectively.</p> foo-mac.user.map<pre><code>+0 root:0\n+501 foo:501\n</code></pre> foo-mac.group.map<pre><code>+0 wheel:0\n+20 staff:20\n+80 admin:80\n</code></pre> <p>Ensure no executable regular file has the <code>com.apple.quarantine</code> extended attribute.</p> <pre><code>sudo -- gfind -- \"${SSH_ALIAS}\" \\\n  -type f -perm /u=x,g=x,o=x -exec xattr -d com.apple.quarantine -- {} +\n</code></pre> <p>Create the archive.</p> <pre><code>&lt; \"${MANIFEST_MODIFIED}\" \\\n    sed -E -- '/^[[:blank:]]*(#|$)/d' \\\n  | tr '\\n' '\\0' \\\n  | sudo -- env -- \\\n      TAR_DIRECTORY=\"${SSH_ALIAS}\" \\\n      TAR_USER_MAP=\"${USER_MAP}\" \\\n      TAR_GROUP_MAP=\"${GROUP_MAP}\" \\\n      ./tar-create.sh \\\n  &gt; \"${TAR_MODIFIED}\"\n</code></pre> <p>Diff the original and modified archives as a sanity check. For example, are ownership and permissions correct?</p> <pre><code>tar-list() {\n  tar -tf \"${1}\" --quoting-style=escape --verbose \\\n    | sed -E -- ':a; /^([^\\t]*\\t){5,}/ b; s/ +/\\t/; ta' \\\n    | cut -f 1-2,6-\n}\n\ndiff -u --color=always &lt;(tar-list \"${TAR_ORIGINAL}\") &lt;(tar-list \"${TAR_MODIFIED}\") | less -RS\n</code></pre> <pre><code>--- /dev/fd/11  2024-01-20 21:36:07.050059294 -0500\n+++ /dev/fd/12  2024-01-20 21:36:07.050322332 -0500\n@@ -1,9 +1,8 @@\n-drwxr-xr-x     root/wheel      Library/\n-drwxr-xr-x     root/wheel      Library/Preferences/\n -rw-r--r--     root/wheel      Library/Preferences/com.soma-zone.LaunchControl.fdautil.plist\n-drwxr-xr-x     root/admin      Users/\n-drwxr-x---     foo/staff       Users/foo/\n-drwxr-xr-x     foo/staff       Users/foo/.config/\n+lrwxr-xr-x     foo/staff       Users/foo/.config/baz -&gt; foo\n+drwxr-xr-x     foo/staff       Users/foo/.config/foo/\n+-rw-r--r--     foo/staff       Users/foo/.config/foo/bar\n+-rw-r--r--     foo/staff       Users/foo/.config/foobar\n drwxr-xr-x     foo/staff       Users/foo/.config/rclone/\n -rw-------     foo/staff       Users/foo/.config/rclone/rclone.conf\n drwxr-xr-x     foo/staff       Users/foo/.config/resticprofile/\n@@ -18,12 +17,7 @@\n -rw-r--r--     foo/staff       Users/foo/.config/resticprofile/profiles.toml\n drwxr-xr-x     foo/staff       Users/foo/.config/resticprofile/status/\n -rw-r--r--     foo/staff       Users/foo/.config/resticprofile/status/foo_mac.json\n-drwx------     foo/staff       Users/foo/Library/\n-drwx------     foo/staff       Users/foo/Library/LaunchAgents/\n -rw-r--r--     foo/staff       Users/foo/Library/LaunchAgents/com.manselmi.resticprofile.foo_mac.backup.plist\n-drwxr-xr-x     root/wheel      usr/\n-drwxr-xr-x     root/wheel      usr/local/\n-drwxr-xr-x     root/wheel      usr/local/bin/\n -rwxr-xr-x     root/wheel      usr/local/bin/exec-rclone\n -rwxr-xr-x     root/wheel      usr/local/bin/exec-resticprofile\n -rwxr-xr-x     root/wheel      usr/local/bin/rclone\n</code></pre> <p>Warning</p> <p>Observe that unlike when we created the original archive on the remote machine, here we choose not to archive the parent directories of items in our manifest. This is because we assume that for every TAR member the corresponding parent directories already exist on the remote system. If this turns out not to be the case, add the required parent directories to the manifest.</p> <p>For example, if you add a new regular file <code>Users/foo/a/b/c.conf</code>, then append all of these lines to the manifest:</p> <pre><code>/Users/foo/a\n/Users/foo/a/b\n/Users/foo/a/b/c.conf\n</code></pre> <p><code>/Users</code> and <code>/Users/foo</code> need not be added because we know those directories already exist on the remote system.</p> <p>Everything looks good, so delete the extracted files.</p> <pre><code>sudo -- rm -fr -- \"${SSH_ALIAS}\"\n</code></pre> <p>Extract the TAR remotely over SSH.</p> <pre><code>pv -W -- \"${TAR_MODIFIED}\" \\\n  | ssh -o RequestTTY=no -- \"${SSH_ALIAS}\" sudo -- env -- \\\n      PATH='/var/manselmi/.prefix/bin:/var/manselmi/.prefix/sw/homebrew/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin' \\\n      TAR_DIRECTORY=/ \\\n      zsh -fc \"$(printf '%q' \"$(&lt; tar-extract.sh)\")\"\n</code></pre>","tags":["shell","ssh"]},{"location":"tmux-env-var-sync/","title":"tmux environment variable sync","text":"","tags":["shell","ssh","tmux"]},{"location":"tmux-env-var-sync/#introduction","title":"Introduction","text":"<p>How to automatically (un)set environment variables when attaching a tmux session.</p> <p>Whenever possible, I avoid SSH agent forwarding and instead rely on <code>ProxyJump</code> if I need to connect to a host via a bastion host. However, sometimes I want to expose my local SSH agent to a remote host so I can clone a Git repo directly onto that host.</p> <p>This can be a pain for tmux users because the remote host looks for the (forwarded) SSH agent via the <code>SSH_AUTH_SOCK</code> environment variable, which will not be set or updated when attaching an existing tmux session created in a previous SSH session.</p> <p>There are plenty of workarounds to be found online, but years ago I was faced with this issue and wanted to implement a more generic solution that would suffice for any environment variable I specify.</p> <p>In short, I do the following:</p> <ol> <li> <p>Before attaching a tmux session, update the specified environment variables in tmux' global    environment to reflect their state in the current shell environment.</p> </li> <li> <p>After attaching a tmux session, update the specified environment variables in the current shell    environment to reflect their state in tmux' global environment.</p> </li> </ol> <p>Here are the relevant snippets from <code>.tmux.conf</code> and <code>.zshrc</code>.</p>","tags":["shell","ssh","tmux"]},{"location":"tmux-env-var-sync/#tmuxconf","title":"<code>~/.tmux.conf</code>","text":"<pre><code># https://www.mankier.com/1/tmux#Options\n# https://www.mankier.com/1/tmux#Global_and_Session_Environment\nset-option -g update-environment \\\n'DISPLAY KRB5CCNAME SSH_AGENT_PID SSH_ASKPASS SSH_AUTH_SOCK SSH_CLIENT SSH_CONNECTION SSH_TTY \\\nSSH_USER_AUTH WINDOWID XAUTHORITY'\n</code></pre>","tags":["shell","ssh","tmux"]},{"location":"tmux-env-var-sync/#zshrc","title":"<code>~/.zshrc</code>","text":"<pre><code>tm() {\n  local SESSION_NAME\n\n  if [[ -v 1 ]]; then\n    SESSION_NAME=\"${1}\"\n  else\n    SESSION_NAME=default\n  fi\n\n  tmux new-session -AD -s \"${SESSION_NAME}\"\n}\n\ntmux_global_update_var() {\n  local VAR=\"${1}\"\n  local VALUE\n\n  if [[ -v 2 ]]; then\n    VALUE=\"${2}\"\n  elif [[ -v \"${VAR}\" ]]; then\n    VALUE=\"${(P)VAR}\"\n  else\n    tmux set-environment -gru -- \"${VAR}\"\n    return\n  fi\n\n  tmux set-environment -g -- \"${VAR}\" \"${VALUE}\"\n}\n\ntmux_shell_update_var() {\n  local VAR=\"${1}\"\n  local TMUX_OUTPUT\n  local TMUX_ERROR_CODE\n\n  TMUX_OUTPUT=\"$(tmux show-environment -gs -- \"${VAR}\" 2&gt; /dev/null)\"\n  TMUX_ERROR_CODE=\"${?}\"\n\n  if [[ \"${TMUX_ERROR_CODE}\" -eq 0 ]]; then\n    eval -- \"${TMUX_OUTPUT}\"\n  else\n    unset -- \"${VAR}\"\n  fi\n}\n\ntmux_update_environment() {\n  if ! whence -p -- tmux &gt; /dev/null; then\n    return\n  fi\n\n  local IFS=$'\\n'\n  local FUNC\n  local VAR\n  local VARS\n\n  if [[ ! -v TMUX ]]; then\n    if ! tmux has-session 2&gt; /dev/null; then\n      return\n    fi\n    FUNC=tmux_global_update_var\n  else\n    FUNC=tmux_shell_update_var\n  fi\n\n  VARS=($(tmux show-options -gv -- update-environment))\n  for VAR in \"${VARS[@]}\"; do\n    eval -- \"${FUNC}\" \"${VAR}\"\n  done\n}\n\n\n# https://zsh.sourceforge.io/Doc/Release/Functions.html#index-preexec\npreexec() {\n  tmux_update_environment\n}\n</code></pre>","tags":["shell","ssh","tmux"]},{"location":"wsl/","title":"Windows Subsystem for Linux","text":"","tags":["ssh","windows"]},{"location":"wsl/#introduction","title":"Introduction","text":"<p>What is the Windows Subsystem for Linux?</p> <p>Windows Subsystem for Linux (WSL) is a feature of Windows that allows you to run a Linux environment on your Windows machine, without the need for a separate virtual machine or dual booting. WSL is designed to provide a seamless and productive experience for developers who want to use both Windows and Linux at the same time.</p> <ul> <li> <p>Use WSL to install and run various Linux distributions, such as Ubuntu, Debian, Kali, and   more. Install Linux distributions and   receive automatic updates from the   Microsoft Store,   import Linux distributions not available in the Microsoft Store,   or build your own customer Linux   distribution.</p> </li> <li> <p>Store files in an isolated Linux file system, specific to the installed distribution.</p> </li> <li> <p>Run command-line tools, such as BASH.</p> </li> <li> <p>Run common BASH command-line tools such as grep, sed, awk, or other ELF-64 binaries.</p> </li> <li> <p>Run Bash scripts and GNU/Linux command-line applications including:</p> <ul> <li> <p>Tools: vim, emacs, tmux</p> </li> <li> <p>Languages: NodeJS,   JavaScript, Python,   Ruby, C/C++, C# &amp; F#, Rust, Go, etc.</p> </li> <li> <p>Services: SSHD,   MySQL, Apache,   lighttpd, MongoDB,   PostgreSQL.</p> </li> </ul> </li> <li> <p>Install additional software using your own GNU/Linux distribution package manager.</p> </li> <li> <p>Invoke Windows applications using a Unix-like command-line shell.</p> </li> <li> <p>Invoke GNU/Linux applications on Windows.</p> </li> <li> <p>Run GNU/Linux graphical   applications integrated   directly to your Windows desktop</p> </li> <li> <p>Use your device GPU to accelerate Machine Learning workloads running on   Linux.</p> </li> </ul> <p>(source)</p>","tags":["ssh","windows"]},{"location":"wsl/#installation","title":"Installation","text":"<ul> <li>Install WSL</li> </ul>","tags":["ssh","windows"]},{"location":"wsl/#tutorials","title":"Tutorials","text":"<ul> <li> <p>Best practices for set up</p> </li> <li> <p>Get started with Linux and Bash</p> </li> <li> <p>Get started with Git</p> </li> <li> <p>Get started with VS Code</p> </li> <li> <p>Get started with Docker remote   containers</p> </li> <li> <p>Set up GPU acceleration (NVIDIA CUDA /   DirectML)</p> </li> </ul>","tags":["ssh","windows"]},{"location":"wsl/#concepts","title":"Concepts","text":"<ul> <li> <p>Working across file systems</p> </li> <li> <p>Advanced settings configuration</p> </li> <li> <p>File access and permissions</p> </li> <li> <p>Networking considerations</p> </li> </ul>","tags":["ssh","windows"]},{"location":"wsl/#configuration","title":"Configuration","text":"","tags":["ssh","windows"]},{"location":"wsl/#vpn","title":"VPN","text":"<p>To make WSL compatible with Cisco AnyConnect VPN, enable mirrored mode networking and DNS tunneling by adding the following to <code>.wslconfig</code> within the <code>%USERPROFILE%</code> directory:</p> %USERPROFILE%\\.wslconfig<pre><code>[wsl2]\nnetworkingMode=mirrored\ndnsTunneling=true\n</code></pre> <p>Learn more about <code>networkingMode</code> and <code>dnsTunneling</code> here.</p> <p>Note</p> <p><code>networkingMode=mirrored</code> and <code>dnsTunneling=true</code> require Windows 11 version 22H2 or higher.</p>","tags":["ssh","windows"]},{"location":"wsl/#vs-code","title":"VS Code","text":"<p>Here we configure VS Code to use the SSH binary provided by WSL, along with the aforementioned SSH config, to have a single SSH config shared by WSL and VS Code.</p> <p>First, install VS Code for your user (not system-wide). When prompted to Select Additional Tasks during installation, be sure to check the Add to PATH option so you can easily open a folder in WSL using the <code>code</code> command. Also, install the Remote Development extension pack.</p> <p>Create a Windows batch file to passthrough SSH invocations to the WSL-provided SSH:</p> %USERPROFILE%\\.ssh\\ssh.bat<pre><code>C:\\Windows\\system32\\wsl.exe ssh %*\n</code></pre> <p>Add the following to VS Code's <code>settings.json</code> within the <code>%APPDATA%\\Code\\User</code> directory:</p> %APPDATA%\\Code\\User\\settings.json<pre><code>{\n    \"remote.SSH.path\": \"C:\\\\Users\\\\manselmi\\\\.ssh\\\\ssh.bat(1)\",\n    \"remote.SSH.remotePlatform\": {\n        \"jump\": \"linux\",\n        \"vps\": \"linux\"\n    },\n    \"security.allowedUNCHosts\": [\"wsl$\", \"wsl.localhost\"]\n}\n</code></pre> <ol> <li>Replace with the expansion of <code>%USERPROFILE%\\.ssh\\ssh.bat</code> for your user, escaping backslashes as    shown here.</li> </ol> <p>Start or restart VS Code. Click the blue Remote Development icon in the lower-left corner, then click Connect Current Window to Host\u2026.</p> <p></p> <p>Type the name of a SSH host alias defined in the SSH config file, such as <code>jump</code>, then press Enter.</p> <p></p> <p>VS Code will then connect and launch a remote session. If you were to select the TERMINAL tab, VS Code would launch a remote shell session.</p> <p></p> <p>Learn more about VS Code Remote Development.</p>","tags":["ssh","windows"]}]}